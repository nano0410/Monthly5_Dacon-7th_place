{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 필요한 package Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sys, warnings\n",
    "import seaborn as sns\n",
    "if not sys.warnoptions: warnings.simplefilter(\"ignore\")\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "pd.set_option('max_columns', 1040, 'max_rows', 140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 변수\n",
    "- id : 구분자\n",
    "- rho : 측정 거리 (단위: mm)\n",
    "- src : 광원 스펙트럼 (650 nm ~ 990 nm)\n",
    "- dst : 측정 스펙트럼 (650 nm ~ 990 nm)\n",
    "- hhb : 디옥시헤모글로빈 농도\n",
    "- hbo2 : 옥시헤모글로빈 농도\n",
    "- ca : 칼슘 농도\n",
    "- na : 나트륨 농도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dst = train.filter(regex='_dst$', axis=1) # dst 데이터만 따로 뺀다.\n",
    "test_dst = test.filter(regex='_dst$', axis=1) # 보간을 하기위해 결측값을 삭제한다.\n",
    "test_dst.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dst.iloc[8].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dst['first'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dst = train_dst[['first','650_dst', '660_dst', '670_dst', '680_dst', '690_dst', '700_dst',\n",
    "       '710_dst', '720_dst', '730_dst', '740_dst', '750_dst', '760_dst',\n",
    "       '770_dst', '780_dst', '790_dst', '800_dst', '810_dst', '820_dst',\n",
    "       '830_dst', '840_dst', '850_dst', '860_dst', '870_dst', '880_dst',\n",
    "       '890_dst', '900_dst', '910_dst', '920_dst', '930_dst', '940_dst',\n",
    "       '950_dst', '960_dst', '970_dst', '980_dst', '990_dst']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dst = train_dst.interpolate(method='linear', axis=1)\n",
    "test_dst = test_dst.interpolate(method='linear', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dst = train_dst.fillna(method='ffill',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dst = test_dst.fillna(method='ffill',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.update(train_dst) # 보간한 데이터를 기존 데이터프레임에 업데이트 한다.\n",
    "test.update(test_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_list = ['hhb', 'hbo2','ca','na']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([train.drop(y_list,axis=1),test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = df.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['src_mean'] = df.loc[:,'650_src':'990_src'].mean(axis=1)\n",
    "df['dst_mean'] = df.loc[:,'650_dst':'990_dst'].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(650,1000,10) : \n",
    "    df['rat_{}'.format(i)] =  (df['{}_dst'.format(i)]) / (df['{}_src'.format(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imag = df.copy()\n",
    "\n",
    "dst_list = list(df.loc[:,'650_dst':'990_dst'].columns)\n",
    "df_real=df[dst_list]\n",
    "df_imag=df[dst_list]\n",
    "\n",
    "for i in tqdm(df_real.index):\n",
    "    df_real.loc[i]=df_real.loc[i] - df_real.loc[i].mean()\n",
    "    df_imag.loc[i]=df_imag.loc[i] - df_imag.loc[i].mean()\n",
    "    \n",
    "    df_real.loc[i] = np.fft.fft(df_real.loc[i], norm='ortho').real\n",
    "    df_imag.loc[i] = np.fft.fft(df_imag.loc[i], norm='ortho').imag\n",
    "\n",
    "real_part=[]\n",
    "imag_part=[]\n",
    "\n",
    "for col in dst_list:\n",
    "    real_part.append(col + '_fft_real')\n",
    "    imag_part.append(col + '_fft_imag')\n",
    "    \n",
    "df_real.columns=real_part\n",
    "df_imag.columns=imag_part\n",
    "df_p = pd.concat((df_real, df_imag), axis=1)\n",
    "\n",
    "\n",
    "df=pd.concat((df, df_p), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src의 스펙트럼 값의 직전값을 빼는 feature 생성\n",
    "for i in range(650,990,10) : \n",
    "    df['src_diff{}'.format(i)] = df['{}_src'.format(i+10)] - df['{}_src'.format(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dst의 스펙트럼 값의 직전값을 빼는 feature 생성\n",
    "for i in range(650,990,10) : \n",
    "    df['dst_diff{}'.format(i)] = df['{}_dst'.format(i+10)] - df['{}_dst'.format(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src, dst의 스펙트럼 값의 직전값의 비율 feature 생성\n",
    "for i in range(650,990,10) : \n",
    "    df['src_rat{}'.format(i)] = df['{}_src'.format(i+10)] / df['{}_src'.format(i)]\n",
    "\n",
    "for i in range(650,990,10) : \n",
    "    df['dst_rat{}'.format(i)] = df['{}_dst'.format(i+10)] / df['{}_dst'.format(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dst의 17번째 뒤의 스팩트럼의 값을 나누는 작업 수행 \n",
    "for i in range(650,830,10) :\n",
    "    df['rev2_dst_rat{}'.format(i)] = df['{}_dst'.format(i)] / df['{}_dst'.format(170+i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dst의 평균값에 거리의 값을 나눔\n",
    "df['rho_dst_mean'] = df['dst_mean'] / df['rho']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src의 평균값에 거리의 값을 나눔 \n",
    "df['rho_src_mean'] = df['src_mean'] / df['rho']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dst의 값을 dst 행의 합으로 나누어 행별 비율로 만듦.\n",
    "dst_rate = df[dst_list].div(df[dst_list].sum(axis=1),axis=0)\n",
    "\n",
    "dst_rate.columns = 'rate_' + dst_rate.columns\n",
    "\n",
    "df = pd.concat([df,dst_rate],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src의 값을 src 행의 합으로 나누어 행별 비율로 만듦.\n",
    "src_rate = df[src_list].div(df[src_list].sum(axis=1),axis=0)\n",
    "\n",
    "src_rate.columns = 'rate_' + src_rate.columns\n",
    "\n",
    "df = pd.concat([df,src_rate],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src의 비율 대비 dst의 비율 비쳐 만듦 \n",
    "for i in range(len(src_rate.columns)) : \n",
    "    df['div_rate{}'.format(i)] = dst_rate.iloc[:,i] / src_rate.iloc[:,i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### features3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dst를 src로 나눈 비율의 평균과 표준편차 만듦 \n",
    "df['rat_mean']= df.loc[:,'rat_650':'rat_990'].replace(np.inf,1).mean(axis=1)\n",
    "df['rat_std']= df.loc[:,'rat_650':'rat_990'].replace(np.inf,1).std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src, dst의 최대값 최소값 표준편차 활용 \n",
    "df['src_max'] = df.iloc[:,1:36].max(axis=1)\n",
    "df['src_min'] = df.iloc[:,1:36].min(axis=1)\n",
    "df['dst_max'] = df.iloc[:,36:71].max(axis=1)\n",
    "df['dst_min'] = df.iloc[:,36:71].min(axis=1)\n",
    "df['src_std'] = df.iloc[:,1:36].std(axis=1)\n",
    "df['dst_std'] = df.iloc[:,36:71].std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src, dst 합 사용 \n",
    "df['src_sum'] = df.iloc[:,2:37].sum(axis=1)\n",
    "df['dst_sum'] = df.iloc[:,37:72].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src의 합과 dst의 합 비율 피쳐 만듦.\n",
    "df['sum_rate'] = df['src_sum'] / df['dst_sum']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 흡광계수 만들기 (빛의 흡수율 활용)\n",
    "\n",
    "src는 광원의 빛의 세기 dst는 측정된 빛의 세기 활용하여 흡광계수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##흡수 계수 \n",
    "for i in range(650,1000,10) : \n",
    "    df['흡수계수_{}'.format(i)] = np.log10(df['{}_src'.format(i)] / df['{}_dst'.format(i)]) / df.rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 흡광계수 평균\n",
    "df['흡수평균'] = df.iloc[:,-35:].replace(np.inf,1).replace(-np.inf,-1).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 흡광계수 표준변차 \n",
    "df['흡수편차'] = df.iloc[:,-35:].replace(np.inf,1).replace(-np.inf,-1).std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 흡광계수 합 \n",
    "df['흡수합'] = df.iloc[:,-35:].replace(np.inf,1).replace(-np.inf,-1).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###reverse dst와 src의 비율 계산한걸 뒤집어 피쳐 생성 \n",
    "for i in range(650,1000,10) : \n",
    "    df['rat_{}_r'.format(i)] =  (df['{}_src'.format(i)]) / (df['{}_dst'.format(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dst와 src의 비율로 만든 값을 나눔. \n",
    "for i in range(len(src_rate.columns)) : \n",
    "    df['div_rate{}_r'.format(i)] =src_rate.iloc[:,i]/ dst_rate.iloc[:,i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dst를 합친것과 src를 합친것을 나눔. \n",
    "df['r_sum_rate'] = df['dst_sum']/ df['src_sum'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 흡수계수의 값을 슬라이딩 하며 비율을 계산함 높은 성능 향상을 보임 \n",
    "for j in range(10,350,10) : \n",
    "    for i in range(650,1000-j,10) : \n",
    "        df['흡수nextrat{}_{}'.format(i,j)] = df['흡수계수_{}'.format(i+j)] / df['흡수계수_{}'.format(i)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  hhb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hhb = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hhb = df_hhb.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm 모델을 돌리는데 dart를 사용 많은 양을 학습시킨 후 최소의 mae num_rounds를 찾아내 재학습 시행 이를 반복함.\n",
    "# 모델을 학습시키며 성능의 감소를 가져오는 피쳐를 찾아내 제거. -> feature selection\n",
    "drop_col = ['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src',\n",
    "        '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst',\n",
    "'960_dst','970_dst','980_dst','990_dst', 'src_diff650', 'src_diff660',\n",
    "       'src_diff670', 'src_diff680', 'src_diff690', 'src_diff700',\n",
    "       'src_diff710', 'src_diff720', 'src_diff730', 'src_diff740',\n",
    "       'src_diff750', 'src_diff760', 'src_diff770', 'src_diff780',\n",
    "       'src_diff790', 'src_diff800', 'src_diff810', 'src_diff820',\n",
    "       'src_diff830', 'src_diff840', 'src_diff850', 'src_diff860',\n",
    "       'src_diff870', 'src_diff880', 'src_diff890', 'src_diff900',\n",
    "       'src_diff910', 'src_diff920', 'src_diff930', 'src_diff940',\n",
    "       'src_diff950', 'src_diff960', 'src_diff970', 'src_diff980',\n",
    "       'dst_diff650', 'dst_diff660', 'dst_diff670', 'dst_diff680',\n",
    "       'dst_diff690', 'dst_diff700', 'dst_diff710', 'dst_diff720',\n",
    "       'dst_diff730', 'dst_diff740', 'dst_diff750', 'dst_diff760',\n",
    "       'dst_diff770', 'dst_diff780', 'dst_diff790', 'dst_diff800',\n",
    "       'dst_diff810', 'dst_diff820', 'dst_diff830', 'dst_diff840',\n",
    "       'dst_diff850', 'dst_diff860', 'dst_diff870', 'dst_diff880',\n",
    "       'dst_diff890', 'dst_diff900', 'dst_diff910', 'dst_diff920',\n",
    "       'dst_diff930', 'dst_diff940', 'dst_diff950', 'dst_diff960',\n",
    "       'dst_diff970', 'dst_diff980']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_hhb.iloc[:train.shape[0],:].drop(columns=drop_col)\n",
    "y_train = train.loc[:,\"hhb\"]\n",
    "X_test = df_hhb.iloc[train.shape[0]:,:].drop(columns=drop_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb#0.842541\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, train_cv, y, y_cv = train_test_split(X_train,y_train, test_size=0.15, random_state=14414)\n",
    "\n",
    "def lgbm_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=288, num_rounds=49748):\n",
    "\n",
    "    param = {}\n",
    "\n",
    "    param['boosting'] = 'dart'\n",
    "    \n",
    "    param['objective'] = 'regression'\n",
    "\n",
    "    param['learning_rate'] = 0.05\n",
    "\n",
    "    param['max_depth'] = 10\n",
    "\n",
    "    param['metric'] = 'mae'\n",
    "    \n",
    "    param['is_training_metric'] = True\n",
    "    \n",
    "    param['min_child_weight'] = 1\n",
    "\n",
    "    param['bagging_fraction'] = 0.8\n",
    "    \n",
    "    param['num_leaves'] = 128\n",
    "\n",
    "    param['feature_fraction'] = 0.8\n",
    "\n",
    "    param['bagging_freq'] = 6\n",
    "    \n",
    "    param['seed'] = seed_val\n",
    "    \n",
    "    param['min_split_gain'] = 0.01\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    test_ds = lgb.Dataset(test_X, label=test_y)\n",
    "\n",
    "    model = lgb.train(param, train_ds, num_rounds,test_ds, early_stopping_rounds=180)\n",
    "\n",
    "    return model\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "\n",
    "model = lgbm_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "\n",
    "lgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#49748 0.510459"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =  model.predict(X_test)\n",
    "submission['hhb'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  hbo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hbo2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hbo2 = df_hbo2.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm 모델을 돌리는데 dart를 사용 많은 양을 학습시킨 후 최소의 mae num_rounds를 찾아내 재학습 시행 이를 반복함.\n",
    "# 모델을 학습시키며 성능의 감소를 가져오는 피쳐를 찾아내 제거.\n",
    "\n",
    "X_train = df_hbo2.iloc[:train.shape[0],:].drop(columns=['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src','rho_src_mean',\n",
    "    '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst', 'src_diff650', 'src_diff660',\n",
    "       'src_diff670', 'src_diff680', 'src_diff690', 'src_diff700',\n",
    "       'src_diff710', 'src_diff720', 'src_diff730', 'src_diff740',\n",
    "       'src_diff750', 'src_diff760', 'src_diff770', 'src_diff780',\n",
    "       'src_diff790', 'src_diff800', 'src_diff810', 'src_diff820',\n",
    "       'src_diff830', 'src_diff840', 'src_diff850', 'src_diff860',\n",
    "       'src_diff870', 'src_diff880', 'src_diff890', 'src_diff900',\n",
    "       'src_diff910', 'src_diff920', 'src_diff930', 'src_diff940',\n",
    "       'src_diff950', 'src_diff960', 'src_diff970', 'src_diff980',\n",
    "       'dst_diff650', 'dst_diff660', 'dst_diff670', 'dst_diff680',\n",
    "       'dst_diff690', 'dst_diff700', 'dst_diff710', 'dst_diff720',\n",
    "       'dst_diff730', 'dst_diff740', 'dst_diff750', 'dst_diff760',\n",
    "       'dst_diff770', 'dst_diff780', 'dst_diff790', 'dst_diff800',\n",
    "       'dst_diff810', 'dst_diff820', 'dst_diff830', 'dst_diff840',\n",
    "       'dst_diff850', 'dst_diff860', 'dst_diff870', 'dst_diff880',\n",
    "       'dst_diff890', 'dst_diff900', 'dst_diff910', 'dst_diff920',\n",
    "       'dst_diff930', 'dst_diff940', 'dst_diff950', 'dst_diff960',\n",
    "       'dst_diff970', 'dst_diff980','src_sum','dst_sum','sum_rate'])\n",
    "y_train = train.loc[:,\"hbo2\"]\n",
    "X_test = df_hbo2.iloc[train.shape[0]:,:].drop(columns=['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src','rho_src_mean',\n",
    "        '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst', 'src_diff650', 'src_diff660',\n",
    "       'src_diff670', 'src_diff680', 'src_diff690', 'src_diff700',\n",
    "       'src_diff710', 'src_diff720', 'src_diff730', 'src_diff740',\n",
    "       'src_diff750', 'src_diff760', 'src_diff770', 'src_diff780',\n",
    "       'src_diff790', 'src_diff800', 'src_diff810', 'src_diff820',\n",
    "       'src_diff830', 'src_diff840', 'src_diff850', 'src_diff860',\n",
    "       'src_diff870', 'src_diff880', 'src_diff890', 'src_diff900',\n",
    "       'src_diff910', 'src_diff920', 'src_diff930', 'src_diff940',\n",
    "       'src_diff950', 'src_diff960', 'src_diff970', 'src_diff980',\n",
    "       'dst_diff650', 'dst_diff660', 'dst_diff670', 'dst_diff680',\n",
    "       'dst_diff690', 'dst_diff700', 'dst_diff710', 'dst_diff720',\n",
    "       'dst_diff730', 'dst_diff740', 'dst_diff750', 'dst_diff760',\n",
    "       'dst_diff770', 'dst_diff780', 'dst_diff790', 'dst_diff800',\n",
    "       'dst_diff810', 'dst_diff820', 'dst_diff830', 'dst_diff840',\n",
    "       'dst_diff850', 'dst_diff860', 'dst_diff870', 'dst_diff880',\n",
    "       'dst_diff890', 'dst_diff900', 'dst_diff910', 'dst_diff920',\n",
    "       'dst_diff930', 'dst_diff940', 'dst_diff950', 'dst_diff960',\n",
    "       'dst_diff970', 'dst_diff980','src_sum','dst_sum','sum_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape#451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb#0.217199\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, train_cv, y, y_cv = train_test_split(X_train,y_train, test_size=0.15, random_state=14414)\n",
    "\n",
    "def lgbm_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=288, num_rounds=15427):\n",
    "\n",
    "    param = {}\n",
    "\n",
    "    param['boosting'] = 'dart'\n",
    "    \n",
    "    param['objective'] = 'regression'\n",
    "\n",
    "    param['learning_rate'] = 0.05\n",
    "\n",
    "    param['max_depth'] = 10\n",
    "\n",
    "    param['metric'] = 'mae'\n",
    "    \n",
    "    param['is_training_metric'] = True\n",
    "    \n",
    "    param['min_child_weight'] = 1\n",
    "\n",
    "    param['bagging_fraction'] = 0.8\n",
    "    \n",
    "    param['num_leaves'] = 128\n",
    "\n",
    "    param['feature_fraction'] = 0.8\n",
    "\n",
    "    param['bagging_freq'] = 6\n",
    "    \n",
    "    param['seed'] = seed_val\n",
    "    \n",
    "    param['min_split_gain'] = 0.01\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    test_ds = lgb.Dataset(test_X, label=test_y)\n",
    "\n",
    "    model = lgb.train(param, train_ds, num_rounds,test_ds, early_stopping_rounds=180)\n",
    "\n",
    "    return model\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "\n",
    "model = lgbm_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "\n",
    "lgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#15427 0.390309"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =  model.predict(X_test)\n",
    "submission['hbo2'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca = df_ca.loc[:,:\"흡수계수_990\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(10,350,10) : \n",
    "    for i in range(650,1000-j,10) : \n",
    "        df_ca['흡수nextrat{}_{}'.format(i,j)] = df_ca['흡수계수_{}'.format(i+j)] / df_ca['흡수계수_{}'.format(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca = df_ca.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm 모델을 돌리는데 dart를 사용 많은 양을 학습시킨 후 최소의 mae num_rounds를 찾아내 재학습 시행 이를 반복함.\n",
    "# 모델을 학습시키며 성능의 감소를 가져오는 피쳐를 찾아내 제거.\n",
    "\n",
    "X_train = df_ca.iloc[:train.shape[0],:].drop(columns=['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src', 'src_diff650', 'src_diff660',\n",
    "       'src_diff670', 'src_diff680', 'src_diff690', 'src_diff700',\n",
    "       'src_diff710', 'src_diff720', 'src_diff730', 'src_diff740',\n",
    "       'src_diff750', 'src_diff760', 'src_diff770', 'src_diff780',\n",
    "       'src_diff790', 'src_diff800', 'src_diff810', 'src_diff820',\n",
    "       'src_diff830', 'src_diff840', 'src_diff850', 'src_diff860',\n",
    "       'src_diff870', 'src_diff880', 'src_diff890', 'src_diff900',\n",
    "       'src_diff910', 'src_diff920', 'src_diff930', 'src_diff940',\n",
    "       'src_diff950', 'src_diff960', 'src_diff970', 'src_diff980',\n",
    "       'dst_diff650', 'dst_diff660', 'dst_diff670', 'dst_diff680',\n",
    "       'dst_diff690', 'dst_diff700', 'dst_diff710', 'dst_diff720',\n",
    "       'dst_diff730', 'dst_diff740', 'dst_diff750', 'dst_diff760',\n",
    "       'dst_diff770', 'dst_diff780', 'dst_diff790', 'dst_diff800',\n",
    "       'dst_diff810', 'dst_diff820', 'dst_diff830', 'dst_diff840',\n",
    "       'dst_diff850', 'dst_diff860', 'dst_diff870', 'dst_diff880',\n",
    "       'dst_diff890', 'dst_diff900', 'dst_diff910', 'dst_diff920',\n",
    "       'dst_diff930', 'dst_diff940', 'dst_diff950', 'dst_diff960',\n",
    "       'dst_diff970', 'dst_diff980','rho_src_mean',\n",
    "        '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst','src_sum','dst_sum','sum_rate'])\n",
    "y_train = train.loc[:,\"ca\"]\n",
    "X_test = df_ca.iloc[train.shape[0]:,:].drop(columns=['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src', 'src_diff650', 'src_diff660',\n",
    "       'src_diff670', 'src_diff680', 'src_diff690', 'src_diff700',\n",
    "       'src_diff710', 'src_diff720', 'src_diff730', 'src_diff740',\n",
    "       'src_diff750', 'src_diff760', 'src_diff770', 'src_diff780',\n",
    "       'src_diff790', 'src_diff800', 'src_diff810', 'src_diff820',\n",
    "       'src_diff830', 'src_diff840', 'src_diff850', 'src_diff860',\n",
    "       'src_diff870', 'src_diff880', 'src_diff890', 'src_diff900',\n",
    "       'src_diff910', 'src_diff920', 'src_diff930', 'src_diff940',\n",
    "       'src_diff950', 'src_diff960', 'src_diff970', 'src_diff980',\n",
    "       'dst_diff650', 'dst_diff660', 'dst_diff670', 'dst_diff680',\n",
    "       'dst_diff690', 'dst_diff700', 'dst_diff710', 'dst_diff720',\n",
    "       'dst_diff730', 'dst_diff740', 'dst_diff750', 'dst_diff760',\n",
    "       'dst_diff770', 'dst_diff780', 'dst_diff790', 'dst_diff800',\n",
    "       'dst_diff810', 'dst_diff820', 'dst_diff830', 'dst_diff840',\n",
    "       'dst_diff850', 'dst_diff860', 'dst_diff870', 'dst_diff880',\n",
    "       'dst_diff890', 'dst_diff900', 'dst_diff910', 'dst_diff920',\n",
    "       'dst_diff930', 'dst_diff940', 'dst_diff950', 'dst_diff960',\n",
    "       'dst_diff970', 'dst_diff980','rho_src_mean',\n",
    "        '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst','src_sum','dst_sum','sum_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb#0.217199\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, train_cv, y, y_cv = train_test_split(X_train,y_train, test_size=0.15, random_state=14414)\n",
    "\n",
    "def lgbm_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=288, num_rounds=46909):\n",
    "\n",
    "    param = {}\n",
    "\n",
    "    param['boosting'] = 'dart'\n",
    "    \n",
    "    param['objective'] = 'regression'\n",
    "\n",
    "    param['learning_rate'] = 0.05\n",
    "\n",
    "    param['max_depth'] = 10\n",
    "\n",
    "    param['metric'] = 'mae'\n",
    "    \n",
    "    param['is_training_metric'] = True\n",
    "    \n",
    "    param['min_child_weight'] = 1\n",
    "\n",
    "    param['bagging_fraction'] = 0.8\n",
    "    \n",
    "    param['num_leaves'] = 128\n",
    "\n",
    "    param['feature_fraction'] = 0.8\n",
    "\n",
    "    param['bagging_freq'] = 6\n",
    "    \n",
    "    param['seed'] = seed_val\n",
    "    \n",
    "    param['min_split_gain'] = 0.01\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    test_ds = lgb.Dataset(test_X, label=test_y)\n",
    "\n",
    "    model = lgb.train(param, train_ds, num_rounds,test_ds, early_stopping_rounds=180)\n",
    "\n",
    "    return model\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "\n",
    "model = lgbm_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "\n",
    "lgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =  model.predict(X_test)\n",
    "submission['ca'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#46909 1.2701"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = df_na.loc[:,:\"흡수계수_990\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = df_na.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(10,350,10) : \n",
    "    for i in range(650,1000-j,10) : \n",
    "        df_na['흡수nextrat{}_{}'.format(i,j)] = df_na['흡수계수_{}'.format(i+j)] / df_na['흡수계수_{}'.format(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm 모델을 돌리는데 dart를 사용 많은 양을 학습시킨 후 최소의 mae num_rounds를 찾아내 재학습 시행 이를 반복함.\n",
    "# 모델을 학습시키며 성능의 감소를 가져오는 피쳐를 찾아내 제거.\n",
    "\n",
    "X_train = df_na.iloc[:train.shape[0],:].drop(columns=['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src','650_dst_fft_imag', '660_dst_fft_imag', '670_dst_fft_imag',\n",
    "       '680_dst_fft_imag', '690_dst_fft_imag', '700_dst_fft_imag',\n",
    "       '710_dst_fft_imag', '720_dst_fft_imag', '730_dst_fft_imag',\n",
    "       '740_dst_fft_imag', '750_dst_fft_imag', '760_dst_fft_imag',\n",
    "       '770_dst_fft_imag', '780_dst_fft_imag', '790_dst_fft_imag',\n",
    "       '800_dst_fft_imag', '810_dst_fft_imag', '820_dst_fft_imag',\n",
    "       '830_dst_fft_imag', '840_dst_fft_imag', '850_dst_fft_imag',\n",
    "       '860_dst_fft_imag', '870_dst_fft_imag', '880_dst_fft_imag',\n",
    "       '890_dst_fft_imag', '900_dst_fft_imag', '910_dst_fft_imag',\n",
    "       '920_dst_fft_imag', '930_dst_fft_imag', '940_dst_fft_imag',\n",
    "       '950_dst_fft_imag', '960_dst_fft_imag', '970_dst_fft_imag',\n",
    "       '980_dst_fft_imag', '990_dst_fft_imag','650_dst_fft_real', '660_dst_fft_real', '670_dst_fft_real',\n",
    "       '680_dst_fft_real', '690_dst_fft_real', '700_dst_fft_real',\n",
    "       '710_dst_fft_real', '720_dst_fft_real', '730_dst_fft_real',\n",
    "       '740_dst_fft_real', '750_dst_fft_real', '760_dst_fft_real',\n",
    "       '770_dst_fft_real', '780_dst_fft_real', '790_dst_fft_real',\n",
    "       '800_dst_fft_real', '810_dst_fft_real', '820_dst_fft_real',\n",
    "       '830_dst_fft_real', '840_dst_fft_real', '850_dst_fft_real',\n",
    "       '860_dst_fft_real', '870_dst_fft_real', '880_dst_fft_real',\n",
    "       '890_dst_fft_real', '900_dst_fft_real', '910_dst_fft_real',\n",
    "       '920_dst_fft_real', '930_dst_fft_real', '940_dst_fft_real',\n",
    "       '950_dst_fft_real', '960_dst_fft_real', '970_dst_fft_real',\n",
    "       '980_dst_fft_real', '990_dst_fft_real','src_diff650', 'src_diff660', 'src_diff670', 'src_diff680',\n",
    "       'src_diff690', 'src_diff700', 'src_diff710', 'src_diff720',\n",
    "       'src_diff730', 'src_diff740', 'src_diff750', 'src_diff760',\n",
    "       'src_diff770', 'src_diff780', 'src_diff790', 'src_diff800',\n",
    "       'src_diff810', 'src_diff820', 'src_diff830', 'src_diff840',\n",
    "       'src_diff850', 'src_diff860', 'src_diff870', 'src_diff880',\n",
    "       'src_diff890', 'src_diff900', 'src_diff910', 'src_diff920',\n",
    "       'src_diff930', 'src_diff940', 'src_diff950', 'src_diff960',\n",
    "       'src_diff970', 'src_diff980', 'dst_diff650', 'dst_diff660',\n",
    "       'dst_diff670', 'dst_diff680', 'dst_diff690', 'dst_diff700',\n",
    "       'dst_diff710', 'dst_diff720', 'dst_diff730', 'dst_diff740',\n",
    "       'dst_diff750', 'dst_diff760', 'dst_diff770', 'dst_diff780',\n",
    "       'dst_diff790', 'dst_diff800', 'dst_diff810', 'dst_diff820',\n",
    "       'dst_diff830', 'dst_diff840', 'dst_diff850', 'dst_diff860',\n",
    "       'dst_diff870', 'dst_diff880', 'dst_diff890', 'dst_diff900',\n",
    "       'dst_diff910', 'dst_diff920', 'dst_diff930', 'dst_diff940',\n",
    "       'dst_diff950', 'dst_diff960', 'dst_diff970', 'dst_diff980','rho_src_mean','rho_dst_mean',\n",
    "        '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst','src_sum','dst_sum','sum_rate'])\n",
    "y_train = train.loc[:,\"na\"]\n",
    "X_test = df_na.iloc[train.shape[0]:,:].drop(columns=['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src','650_dst_fft_imag', '660_dst_fft_imag', '670_dst_fft_imag',\n",
    "       '680_dst_fft_imag', '690_dst_fft_imag', '700_dst_fft_imag',\n",
    "       '710_dst_fft_imag', '720_dst_fft_imag', '730_dst_fft_imag',\n",
    "       '740_dst_fft_imag', '750_dst_fft_imag', '760_dst_fft_imag',\n",
    "       '770_dst_fft_imag', '780_dst_fft_imag', '790_dst_fft_imag',\n",
    "       '800_dst_fft_imag', '810_dst_fft_imag', '820_dst_fft_imag',\n",
    "       '830_dst_fft_imag', '840_dst_fft_imag', '850_dst_fft_imag',\n",
    "       '860_dst_fft_imag', '870_dst_fft_imag', '880_dst_fft_imag',\n",
    "       '890_dst_fft_imag', '900_dst_fft_imag', '910_dst_fft_imag',\n",
    "       '920_dst_fft_imag', '930_dst_fft_imag', '940_dst_fft_imag',\n",
    "       '950_dst_fft_imag', '960_dst_fft_imag', '970_dst_fft_imag',\n",
    "       '980_dst_fft_imag', '990_dst_fft_imag','650_dst_fft_real', '660_dst_fft_real', '670_dst_fft_real',\n",
    "       '680_dst_fft_real', '690_dst_fft_real', '700_dst_fft_real',\n",
    "       '710_dst_fft_real', '720_dst_fft_real', '730_dst_fft_real',\n",
    "       '740_dst_fft_real', '750_dst_fft_real', '760_dst_fft_real',\n",
    "       '770_dst_fft_real', '780_dst_fft_real', '790_dst_fft_real',\n",
    "       '800_dst_fft_real', '810_dst_fft_real', '820_dst_fft_real',\n",
    "       '830_dst_fft_real', '840_dst_fft_real', '850_dst_fft_real',\n",
    "       '860_dst_fft_real', '870_dst_fft_real', '880_dst_fft_real',\n",
    "       '890_dst_fft_real', '900_dst_fft_real', '910_dst_fft_real',\n",
    "       '920_dst_fft_real', '930_dst_fft_real', '940_dst_fft_real',\n",
    "       '950_dst_fft_real', '960_dst_fft_real', '970_dst_fft_real',\n",
    "       '980_dst_fft_real', '990_dst_fft_real','src_diff650', 'src_diff660', 'src_diff670', 'src_diff680',\n",
    "       'src_diff690', 'src_diff700', 'src_diff710', 'src_diff720',\n",
    "       'src_diff730', 'src_diff740', 'src_diff750', 'src_diff760',\n",
    "       'src_diff770', 'src_diff780', 'src_diff790', 'src_diff800',\n",
    "       'src_diff810', 'src_diff820', 'src_diff830', 'src_diff840',\n",
    "       'src_diff850', 'src_diff860', 'src_diff870', 'src_diff880',\n",
    "       'src_diff890', 'src_diff900', 'src_diff910', 'src_diff920',\n",
    "       'src_diff930', 'src_diff940', 'src_diff950', 'src_diff960',\n",
    "       'src_diff970', 'src_diff980', 'dst_diff650', 'dst_diff660',\n",
    "       'dst_diff670', 'dst_diff680', 'dst_diff690', 'dst_diff700',\n",
    "       'dst_diff710', 'dst_diff720', 'dst_diff730', 'dst_diff740',\n",
    "       'dst_diff750', 'dst_diff760', 'dst_diff770', 'dst_diff780',\n",
    "       'dst_diff790', 'dst_diff800', 'dst_diff810', 'dst_diff820',\n",
    "       'dst_diff830', 'dst_diff840', 'dst_diff850', 'dst_diff860',\n",
    "       'dst_diff870', 'dst_diff880', 'dst_diff890', 'dst_diff900',\n",
    "       'dst_diff910', 'dst_diff920', 'dst_diff930', 'dst_diff940',\n",
    "       'dst_diff950', 'dst_diff960', 'dst_diff970', 'dst_diff980','rho_src_mean','rho_dst_mean',\n",
    "                    '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst','src_sum','dst_sum','sum_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb#0.217199\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, train_cv, y, y_cv = train_test_split(X_train,y_train, test_size=0.15, random_state=14414)\n",
    "\n",
    "def lgbm_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=288, num_rounds=24713):\n",
    "\n",
    "    param = {}\n",
    "\n",
    "    param['boosting'] = 'dart'\n",
    "    \n",
    "    param['objective'] = 'regression'\n",
    "\n",
    "    param['learning_rate'] = 0.05\n",
    "\n",
    "    param['max_depth'] = 10\n",
    "\n",
    "    param['metric'] = 'mae'\n",
    "    \n",
    "    param['is_training_metric'] = True\n",
    "    \n",
    "    param['min_child_weight'] = 1\n",
    "\n",
    "    param['bagging_fraction'] = 0.8\n",
    "    \n",
    "    param['num_leaves'] = 128\n",
    "\n",
    "    param['feature_fraction'] = 0.8\n",
    "\n",
    "    param['bagging_freq'] = 6\n",
    "    \n",
    "    param['seed'] = seed_val\n",
    "    \n",
    "    param['min_split_gain'] = 0.01\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    test_ds = lgb.Dataset(test_X, label=test_y)\n",
    "\n",
    "    model = lgb.train(param, train_ds, num_rounds,test_ds, early_stopping_rounds=180)\n",
    "\n",
    "    return model\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "\n",
    "model = lgbm_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "\n",
    "lgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24713 1.02729"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test =  model.predict(X_test)\n",
    "submission['na'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_zero(x) : \n",
    "    if x <= 0 : \n",
    "        return(0)\n",
    "    else : return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in submission.columns : \n",
    "    submission[i] = submission[i].apply(to_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('edit_dst.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = submission.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  hhb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hhb = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hhb = df_hhb.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm 모델을 돌리는데 dart를 사용 많은 양을 학습시킨 후 최소의 mae num_rounds를 찾아내 재학습 시행 이를 반복함.\n",
    "# 모델을 학습시키며 성능의 감소를 가져오는 피쳐를 찾아내 제거.\n",
    "\n",
    "drop_col = ['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src',\n",
    "        '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst',\n",
    "'960_dst','970_dst','980_dst','990_dst', 'src_diff650', 'src_diff660',\n",
    "       'src_diff670', 'src_diff680', 'src_diff690', 'src_diff700',\n",
    "       'src_diff710', 'src_diff720', 'src_diff730', 'src_diff740',\n",
    "       'src_diff750', 'src_diff760', 'src_diff770', 'src_diff780',\n",
    "       'src_diff790', 'src_diff800', 'src_diff810', 'src_diff820',\n",
    "       'src_diff830', 'src_diff840', 'src_diff850', 'src_diff860',\n",
    "       'src_diff870', 'src_diff880', 'src_diff890', 'src_diff900',\n",
    "       'src_diff910', 'src_diff920', 'src_diff930', 'src_diff940',\n",
    "       'src_diff950', 'src_diff960', 'src_diff970', 'src_diff980',\n",
    "       'dst_diff650', 'dst_diff660', 'dst_diff670', 'dst_diff680',\n",
    "       'dst_diff690', 'dst_diff700', 'dst_diff710', 'dst_diff720',\n",
    "       'dst_diff730', 'dst_diff740', 'dst_diff750', 'dst_diff760',\n",
    "       'dst_diff770', 'dst_diff780', 'dst_diff790', 'dst_diff800',\n",
    "       'dst_diff810', 'dst_diff820', 'dst_diff830', 'dst_diff840',\n",
    "       'dst_diff850', 'dst_diff860', 'dst_diff870', 'dst_diff880',\n",
    "       'dst_diff890', 'dst_diff900', 'dst_diff910', 'dst_diff920',\n",
    "       'dst_diff930', 'dst_diff940', 'dst_diff950', 'dst_diff960',\n",
    "       'dst_diff970', 'dst_diff980']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_hhb.iloc[:train.shape[0],:].drop(columns=drop_col)\n",
    "y_train = train.loc[:,\"hhb\"]\n",
    "X_test = df_hhb.iloc[train.shape[0]:,:].drop(columns=drop_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb#0.842541\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, train_cv, y, y_cv = train_test_split(X_train,y_train, test_size=0.15, random_state=998)\n",
    "\n",
    "def lgbm_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=288, num_rounds=52956):\n",
    "\n",
    "    param = {}\n",
    "\n",
    "    param['boosting'] = 'dart'\n",
    "    \n",
    "    param['objective'] = 'regression'\n",
    "\n",
    "    param['learning_rate'] = 0.05\n",
    "\n",
    "    param['max_depth'] = 10\n",
    "\n",
    "    param['metric'] = 'mae'\n",
    "    \n",
    "    param['is_training_metric'] = True\n",
    "    \n",
    "    param['min_child_weight'] = 1\n",
    "\n",
    "    param['bagging_fraction'] = 0.8\n",
    "    \n",
    "    param['num_leaves'] = 128\n",
    "\n",
    "    param['feature_fraction'] = 0.8\n",
    "\n",
    "    param['bagging_freq'] = 6\n",
    "    \n",
    "    param['seed'] = seed_val\n",
    "    \n",
    "    param['min_split_gain'] = 0.01\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    test_ds = lgb.Dataset(test_X, label=test_y)\n",
    "\n",
    "    model = lgb.train(param, train_ds, num_rounds,test_ds, early_stopping_rounds=180)\n",
    "\n",
    "    return model\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "\n",
    "model = lgbm_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "\n",
    "lgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =  model.predict(X_test)\n",
    "submission['hhb'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  hbo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hbo2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hbo2 = df_hbo2.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm 모델을 돌리는데 dart를 사용 많은 양을 학습시킨 후 최소의 mae num_rounds를 찾아내 재학습 시행 이를 반복함.\n",
    "# 모델을 학습시키며 성능의 감소를 가져오는 피쳐를 찾아내 제거.\n",
    "\n",
    "X_train = df_hbo2.iloc[:train.shape[0],:].drop(columns=['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src','rho_src_mean',\n",
    "    '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst', 'src_diff650', 'src_diff660',\n",
    "       'src_diff670', 'src_diff680', 'src_diff690', 'src_diff700',\n",
    "       'src_diff710', 'src_diff720', 'src_diff730', 'src_diff740',\n",
    "       'src_diff750', 'src_diff760', 'src_diff770', 'src_diff780',\n",
    "       'src_diff790', 'src_diff800', 'src_diff810', 'src_diff820',\n",
    "       'src_diff830', 'src_diff840', 'src_diff850', 'src_diff860',\n",
    "       'src_diff870', 'src_diff880', 'src_diff890', 'src_diff900',\n",
    "       'src_diff910', 'src_diff920', 'src_diff930', 'src_diff940',\n",
    "       'src_diff950', 'src_diff960', 'src_diff970', 'src_diff980',\n",
    "       'dst_diff650', 'dst_diff660', 'dst_diff670', 'dst_diff680',\n",
    "       'dst_diff690', 'dst_diff700', 'dst_diff710', 'dst_diff720',\n",
    "       'dst_diff730', 'dst_diff740', 'dst_diff750', 'dst_diff760',\n",
    "       'dst_diff770', 'dst_diff780', 'dst_diff790', 'dst_diff800',\n",
    "       'dst_diff810', 'dst_diff820', 'dst_diff830', 'dst_diff840',\n",
    "       'dst_diff850', 'dst_diff860', 'dst_diff870', 'dst_diff880',\n",
    "       'dst_diff890', 'dst_diff900', 'dst_diff910', 'dst_diff920',\n",
    "       'dst_diff930', 'dst_diff940', 'dst_diff950', 'dst_diff960',\n",
    "       'dst_diff970', 'dst_diff980','src_sum','dst_sum','sum_rate'])\n",
    "y_train = train.loc[:,\"hbo2\"]\n",
    "X_test = df_hbo2.iloc[train.shape[0]:,:].drop(columns=['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src','rho_src_mean',\n",
    "        '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst', 'src_diff650', 'src_diff660',\n",
    "       'src_diff670', 'src_diff680', 'src_diff690', 'src_diff700',\n",
    "       'src_diff710', 'src_diff720', 'src_diff730', 'src_diff740',\n",
    "       'src_diff750', 'src_diff760', 'src_diff770', 'src_diff780',\n",
    "       'src_diff790', 'src_diff800', 'src_diff810', 'src_diff820',\n",
    "       'src_diff830', 'src_diff840', 'src_diff850', 'src_diff860',\n",
    "       'src_diff870', 'src_diff880', 'src_diff890', 'src_diff900',\n",
    "       'src_diff910', 'src_diff920', 'src_diff930', 'src_diff940',\n",
    "       'src_diff950', 'src_diff960', 'src_diff970', 'src_diff980',\n",
    "       'dst_diff650', 'dst_diff660', 'dst_diff670', 'dst_diff680',\n",
    "       'dst_diff690', 'dst_diff700', 'dst_diff710', 'dst_diff720',\n",
    "       'dst_diff730', 'dst_diff740', 'dst_diff750', 'dst_diff760',\n",
    "       'dst_diff770', 'dst_diff780', 'dst_diff790', 'dst_diff800',\n",
    "       'dst_diff810', 'dst_diff820', 'dst_diff830', 'dst_diff840',\n",
    "       'dst_diff850', 'dst_diff860', 'dst_diff870', 'dst_diff880',\n",
    "       'dst_diff890', 'dst_diff900', 'dst_diff910', 'dst_diff920',\n",
    "       'dst_diff930', 'dst_diff940', 'dst_diff950', 'dst_diff960',\n",
    "       'dst_diff970', 'dst_diff980','src_sum','dst_sum','sum_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb#0.217199\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, train_cv, y, y_cv = train_test_split(X_train,y_train, test_size=0.15, random_state=998)\n",
    "\n",
    "def lgbm_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=288, num_rounds=32581):\n",
    "\n",
    "    param = {}\n",
    "\n",
    "    param['boosting'] = 'dart'\n",
    "    \n",
    "    param['objective'] = 'regression'\n",
    "\n",
    "    param['learning_rate'] = 0.05\n",
    "\n",
    "    param['max_depth'] = 10\n",
    "\n",
    "    param['metric'] = 'mae'\n",
    "    \n",
    "    param['is_training_metric'] = True\n",
    "    \n",
    "    param['min_child_weight'] = 1\n",
    "\n",
    "    param['bagging_fraction'] = 0.8\n",
    "    \n",
    "    param['num_leaves'] = 128\n",
    "\n",
    "    param['feature_fraction'] = 0.8\n",
    "\n",
    "    param['bagging_freq'] = 6\n",
    "    \n",
    "    param['seed'] = seed_val\n",
    "    \n",
    "    param['min_split_gain'] = 0.01\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    test_ds = lgb.Dataset(test_X, label=test_y)\n",
    "\n",
    "    model = lgb.train(param, train_ds, num_rounds,test_ds, early_stopping_rounds=180)\n",
    "\n",
    "    return model\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "\n",
    "model = lgbm_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "\n",
    "lgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =  model.predict(X_test)\n",
    "submission['hbo2'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca = df_ca.loc[:,:\"흡수계수_990\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(10,350,10) : \n",
    "    for i in range(650,1000-j,10) : \n",
    "        df_ca['흡수nextrat{}_{}'.format(i,j)] = df_ca['흡수계수_{}'.format(i+j)] / df_ca['흡수계수_{}'.format(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca = df_ca.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm 모델을 돌리는데 dart를 사용 많은 양을 학습시킨 후 최소의 mae num_rounds를 찾아내 재학습 시행 이를 반복함.\n",
    "# 모델을 학습시키며 성능의 감소를 가져오는 피쳐를 찾아내 제거.\n",
    "\n",
    "X_train = df_ca.iloc[:train.shape[0],:].drop(columns=['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src', 'src_diff650', 'src_diff660',\n",
    "       'src_diff670', 'src_diff680', 'src_diff690', 'src_diff700',\n",
    "       'src_diff710', 'src_diff720', 'src_diff730', 'src_diff740',\n",
    "       'src_diff750', 'src_diff760', 'src_diff770', 'src_diff780',\n",
    "       'src_diff790', 'src_diff800', 'src_diff810', 'src_diff820',\n",
    "       'src_diff830', 'src_diff840', 'src_diff850', 'src_diff860',\n",
    "       'src_diff870', 'src_diff880', 'src_diff890', 'src_diff900',\n",
    "       'src_diff910', 'src_diff920', 'src_diff930', 'src_diff940',\n",
    "       'src_diff950', 'src_diff960', 'src_diff970', 'src_diff980',\n",
    "       'dst_diff650', 'dst_diff660', 'dst_diff670', 'dst_diff680',\n",
    "       'dst_diff690', 'dst_diff700', 'dst_diff710', 'dst_diff720',\n",
    "       'dst_diff730', 'dst_diff740', 'dst_diff750', 'dst_diff760',\n",
    "       'dst_diff770', 'dst_diff780', 'dst_diff790', 'dst_diff800',\n",
    "       'dst_diff810', 'dst_diff820', 'dst_diff830', 'dst_diff840',\n",
    "       'dst_diff850', 'dst_diff860', 'dst_diff870', 'dst_diff880',\n",
    "       'dst_diff890', 'dst_diff900', 'dst_diff910', 'dst_diff920',\n",
    "       'dst_diff930', 'dst_diff940', 'dst_diff950', 'dst_diff960',\n",
    "       'dst_diff970', 'dst_diff980','rho_src_mean',\n",
    "        '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst','src_sum','dst_sum','sum_rate'])\n",
    "y_train = train.loc[:,\"ca\"]\n",
    "X_test = df_ca.iloc[train.shape[0]:,:].drop(columns=['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src', 'src_diff650', 'src_diff660',\n",
    "       'src_diff670', 'src_diff680', 'src_diff690', 'src_diff700',\n",
    "       'src_diff710', 'src_diff720', 'src_diff730', 'src_diff740',\n",
    "       'src_diff750', 'src_diff760', 'src_diff770', 'src_diff780',\n",
    "       'src_diff790', 'src_diff800', 'src_diff810', 'src_diff820',\n",
    "       'src_diff830', 'src_diff840', 'src_diff850', 'src_diff860',\n",
    "       'src_diff870', 'src_diff880', 'src_diff890', 'src_diff900',\n",
    "       'src_diff910', 'src_diff920', 'src_diff930', 'src_diff940',\n",
    "       'src_diff950', 'src_diff960', 'src_diff970', 'src_diff980',\n",
    "       'dst_diff650', 'dst_diff660', 'dst_diff670', 'dst_diff680',\n",
    "       'dst_diff690', 'dst_diff700', 'dst_diff710', 'dst_diff720',\n",
    "       'dst_diff730', 'dst_diff740', 'dst_diff750', 'dst_diff760',\n",
    "       'dst_diff770', 'dst_diff780', 'dst_diff790', 'dst_diff800',\n",
    "       'dst_diff810', 'dst_diff820', 'dst_diff830', 'dst_diff840',\n",
    "       'dst_diff850', 'dst_diff860', 'dst_diff870', 'dst_diff880',\n",
    "       'dst_diff890', 'dst_diff900', 'dst_diff910', 'dst_diff920',\n",
    "       'dst_diff930', 'dst_diff940', 'dst_diff950', 'dst_diff960',\n",
    "       'dst_diff970', 'dst_diff980','rho_src_mean',\n",
    "        '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst','src_sum','dst_sum','sum_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb#0.217199\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, train_cv, y, y_cv = train_test_split(X_train,y_train, test_size=0.15, random_state=998)\n",
    "\n",
    "def lgbm_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=288, num_rounds=30530):\n",
    "\n",
    "    param = {}\n",
    "\n",
    "    param['boosting'] = 'dart'\n",
    "    \n",
    "    param['objective'] = 'regression'\n",
    "\n",
    "    param['learning_rate'] = 0.05\n",
    "\n",
    "    param['max_depth'] = 10\n",
    "\n",
    "    param['metric'] = 'mae'\n",
    "    \n",
    "    param['is_training_metric'] = True\n",
    "    \n",
    "    param['min_child_weight'] = 1\n",
    "\n",
    "    param['bagging_fraction'] = 0.8\n",
    "    \n",
    "    param['num_leaves'] = 128\n",
    "\n",
    "    param['feature_fraction'] = 0.8\n",
    "\n",
    "    param['bagging_freq'] = 6\n",
    "    \n",
    "    param['seed'] = seed_val\n",
    "    \n",
    "    param['min_split_gain'] = 0.01\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    test_ds = lgb.Dataset(test_X, label=test_y)\n",
    "\n",
    "    model = lgb.train(param, train_ds, num_rounds,test_ds, early_stopping_rounds=180)\n",
    "\n",
    "    return model\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "\n",
    "model = lgbm_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "\n",
    "lgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =  model.predict(X_test)\n",
    "submission['ca'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = df_na.loc[:,:\"흡수계수_990\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = df_na.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(10,350,10) : \n",
    "    for i in range(650,1000-j,10) : \n",
    "        df_na['흡수nextrat{}_{}'.format(i,j)] = df_na['흡수계수_{}'.format(i+j)] / df_na['흡수계수_{}'.format(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm 모델을 돌리는데 dart를 사용 많은 양을 학습시킨 후 최소의 mae num_rounds를 찾아내 재학습 시행 이를 반복함.\n",
    "# 모델을 학습시키며 성능의 감소를 가져오는 피쳐를 찾아내 제거.\n",
    "\n",
    "X_train = df_na.iloc[:train.shape[0],:].drop(columns=['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src','650_dst_fft_imag', '660_dst_fft_imag', '670_dst_fft_imag',\n",
    "       '680_dst_fft_imag', '690_dst_fft_imag', '700_dst_fft_imag',\n",
    "       '710_dst_fft_imag', '720_dst_fft_imag', '730_dst_fft_imag',\n",
    "       '740_dst_fft_imag', '750_dst_fft_imag', '760_dst_fft_imag',\n",
    "       '770_dst_fft_imag', '780_dst_fft_imag', '790_dst_fft_imag',\n",
    "       '800_dst_fft_imag', '810_dst_fft_imag', '820_dst_fft_imag',\n",
    "       '830_dst_fft_imag', '840_dst_fft_imag', '850_dst_fft_imag',\n",
    "       '860_dst_fft_imag', '870_dst_fft_imag', '880_dst_fft_imag',\n",
    "       '890_dst_fft_imag', '900_dst_fft_imag', '910_dst_fft_imag',\n",
    "       '920_dst_fft_imag', '930_dst_fft_imag', '940_dst_fft_imag',\n",
    "       '950_dst_fft_imag', '960_dst_fft_imag', '970_dst_fft_imag',\n",
    "       '980_dst_fft_imag', '990_dst_fft_imag','650_dst_fft_real', '660_dst_fft_real', '670_dst_fft_real',\n",
    "       '680_dst_fft_real', '690_dst_fft_real', '700_dst_fft_real',\n",
    "       '710_dst_fft_real', '720_dst_fft_real', '730_dst_fft_real',\n",
    "       '740_dst_fft_real', '750_dst_fft_real', '760_dst_fft_real',\n",
    "       '770_dst_fft_real', '780_dst_fft_real', '790_dst_fft_real',\n",
    "       '800_dst_fft_real', '810_dst_fft_real', '820_dst_fft_real',\n",
    "       '830_dst_fft_real', '840_dst_fft_real', '850_dst_fft_real',\n",
    "       '860_dst_fft_real', '870_dst_fft_real', '880_dst_fft_real',\n",
    "       '890_dst_fft_real', '900_dst_fft_real', '910_dst_fft_real',\n",
    "       '920_dst_fft_real', '930_dst_fft_real', '940_dst_fft_real',\n",
    "       '950_dst_fft_real', '960_dst_fft_real', '970_dst_fft_real',\n",
    "       '980_dst_fft_real', '990_dst_fft_real','src_diff650', 'src_diff660', 'src_diff670', 'src_diff680',\n",
    "       'src_diff690', 'src_diff700', 'src_diff710', 'src_diff720',\n",
    "       'src_diff730', 'src_diff740', 'src_diff750', 'src_diff760',\n",
    "       'src_diff770', 'src_diff780', 'src_diff790', 'src_diff800',\n",
    "       'src_diff810', 'src_diff820', 'src_diff830', 'src_diff840',\n",
    "       'src_diff850', 'src_diff860', 'src_diff870', 'src_diff880',\n",
    "       'src_diff890', 'src_diff900', 'src_diff910', 'src_diff920',\n",
    "       'src_diff930', 'src_diff940', 'src_diff950', 'src_diff960',\n",
    "       'src_diff970', 'src_diff980', 'dst_diff650', 'dst_diff660',\n",
    "       'dst_diff670', 'dst_diff680', 'dst_diff690', 'dst_diff700',\n",
    "       'dst_diff710', 'dst_diff720', 'dst_diff730', 'dst_diff740',\n",
    "       'dst_diff750', 'dst_diff760', 'dst_diff770', 'dst_diff780',\n",
    "       'dst_diff790', 'dst_diff800', 'dst_diff810', 'dst_diff820',\n",
    "       'dst_diff830', 'dst_diff840', 'dst_diff850', 'dst_diff860',\n",
    "       'dst_diff870', 'dst_diff880', 'dst_diff890', 'dst_diff900',\n",
    "       'dst_diff910', 'dst_diff920', 'dst_diff930', 'dst_diff940',\n",
    "       'dst_diff950', 'dst_diff960', 'dst_diff970', 'dst_diff980','rho_src_mean','rho_dst_mean',\n",
    "        '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst','src_sum','dst_sum','sum_rate'])\n",
    "y_train = train.loc[:,\"na\"]\n",
    "X_test = df_na.iloc[train.shape[0]:,:].drop(columns=['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src','650_dst_fft_imag', '660_dst_fft_imag', '670_dst_fft_imag',\n",
    "       '680_dst_fft_imag', '690_dst_fft_imag', '700_dst_fft_imag',\n",
    "       '710_dst_fft_imag', '720_dst_fft_imag', '730_dst_fft_imag',\n",
    "       '740_dst_fft_imag', '750_dst_fft_imag', '760_dst_fft_imag',\n",
    "       '770_dst_fft_imag', '780_dst_fft_imag', '790_dst_fft_imag',\n",
    "       '800_dst_fft_imag', '810_dst_fft_imag', '820_dst_fft_imag',\n",
    "       '830_dst_fft_imag', '840_dst_fft_imag', '850_dst_fft_imag',\n",
    "       '860_dst_fft_imag', '870_dst_fft_imag', '880_dst_fft_imag',\n",
    "       '890_dst_fft_imag', '900_dst_fft_imag', '910_dst_fft_imag',\n",
    "       '920_dst_fft_imag', '930_dst_fft_imag', '940_dst_fft_imag',\n",
    "       '950_dst_fft_imag', '960_dst_fft_imag', '970_dst_fft_imag',\n",
    "       '980_dst_fft_imag', '990_dst_fft_imag','650_dst_fft_real', '660_dst_fft_real', '670_dst_fft_real',\n",
    "       '680_dst_fft_real', '690_dst_fft_real', '700_dst_fft_real',\n",
    "       '710_dst_fft_real', '720_dst_fft_real', '730_dst_fft_real',\n",
    "       '740_dst_fft_real', '750_dst_fft_real', '760_dst_fft_real',\n",
    "       '770_dst_fft_real', '780_dst_fft_real', '790_dst_fft_real',\n",
    "       '800_dst_fft_real', '810_dst_fft_real', '820_dst_fft_real',\n",
    "       '830_dst_fft_real', '840_dst_fft_real', '850_dst_fft_real',\n",
    "       '860_dst_fft_real', '870_dst_fft_real', '880_dst_fft_real',\n",
    "       '890_dst_fft_real', '900_dst_fft_real', '910_dst_fft_real',\n",
    "       '920_dst_fft_real', '930_dst_fft_real', '940_dst_fft_real',\n",
    "       '950_dst_fft_real', '960_dst_fft_real', '970_dst_fft_real',\n",
    "       '980_dst_fft_real', '990_dst_fft_real','src_diff650', 'src_diff660', 'src_diff670', 'src_diff680',\n",
    "       'src_diff690', 'src_diff700', 'src_diff710', 'src_diff720',\n",
    "       'src_diff730', 'src_diff740', 'src_diff750', 'src_diff760',\n",
    "       'src_diff770', 'src_diff780', 'src_diff790', 'src_diff800',\n",
    "       'src_diff810', 'src_diff820', 'src_diff830', 'src_diff840',\n",
    "       'src_diff850', 'src_diff860', 'src_diff870', 'src_diff880',\n",
    "       'src_diff890', 'src_diff900', 'src_diff910', 'src_diff920',\n",
    "       'src_diff930', 'src_diff940', 'src_diff950', 'src_diff960',\n",
    "       'src_diff970', 'src_diff980', 'dst_diff650', 'dst_diff660',\n",
    "       'dst_diff670', 'dst_diff680', 'dst_diff690', 'dst_diff700',\n",
    "       'dst_diff710', 'dst_diff720', 'dst_diff730', 'dst_diff740',\n",
    "       'dst_diff750', 'dst_diff760', 'dst_diff770', 'dst_diff780',\n",
    "       'dst_diff790', 'dst_diff800', 'dst_diff810', 'dst_diff820',\n",
    "       'dst_diff830', 'dst_diff840', 'dst_diff850', 'dst_diff860',\n",
    "       'dst_diff870', 'dst_diff880', 'dst_diff890', 'dst_diff900',\n",
    "       'dst_diff910', 'dst_diff920', 'dst_diff930', 'dst_diff940',\n",
    "       'dst_diff950', 'dst_diff960', 'dst_diff970', 'dst_diff980','rho_src_mean','rho_dst_mean',\n",
    "                    '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst','src_sum','dst_sum','sum_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb#0.217199\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, train_cv, y, y_cv = train_test_split(X_train,y_train, test_size=0.15, random_state=998)\n",
    "\n",
    "def lgbm_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=288, num_rounds=11274):\n",
    "\n",
    "    param = {}\n",
    "\n",
    "    param['boosting'] = 'dart'\n",
    "    \n",
    "    param['objective'] = 'regression'\n",
    "\n",
    "    param['learning_rate'] = 0.05\n",
    "\n",
    "    param['max_depth'] = 10\n",
    "\n",
    "    param['metric'] = 'mae'\n",
    "    \n",
    "    param['is_training_metric'] = True\n",
    "    \n",
    "    param['min_child_weight'] = 1\n",
    "\n",
    "    param['bagging_fraction'] = 0.8\n",
    "    \n",
    "    param['num_leaves'] = 128\n",
    "\n",
    "    param['feature_fraction'] = 0.8\n",
    "\n",
    "    param['bagging_freq'] = 6\n",
    "    \n",
    "    param['seed'] = seed_val\n",
    "    \n",
    "    param['min_split_gain'] = 0.01\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    test_ds = lgb.Dataset(test_X, label=test_y)\n",
    "\n",
    "    model = lgb.train(param, train_ds, num_rounds,test_ds, early_stopping_rounds=180)\n",
    "\n",
    "    return model\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "\n",
    "model = lgbm_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "\n",
    "lgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test =  model.predict(X_test)\n",
    "submission['na'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_zero(x) : \n",
    "    if x <= 0 : \n",
    "        return(0)\n",
    "    else : return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in submission.columns : \n",
    "    submission[i] = submission[i].apply(to_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('edit_dst_998.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = submission.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## case3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  hhb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hhb = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hhb = df_hhb.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm 모델을 돌리는데 dart를 사용 많은 양을 학습시킨 후 최소의 mae num_rounds를 찾아내 재학습 시행 이를 반복함.\n",
    "# 모델을 학습시키며 성능의 감소를 가져오는 피쳐를 찾아내 제거.\n",
    "\n",
    "drop_col = ['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src',\n",
    "        '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst',\n",
    "'960_dst','970_dst','980_dst','990_dst', 'src_diff650', 'src_diff660',\n",
    "       'src_diff670', 'src_diff680', 'src_diff690', 'src_diff700',\n",
    "       'src_diff710', 'src_diff720', 'src_diff730', 'src_diff740',\n",
    "       'src_diff750', 'src_diff760', 'src_diff770', 'src_diff780',\n",
    "       'src_diff790', 'src_diff800', 'src_diff810', 'src_diff820',\n",
    "       'src_diff830', 'src_diff840', 'src_diff850', 'src_diff860',\n",
    "       'src_diff870', 'src_diff880', 'src_diff890', 'src_diff900',\n",
    "       'src_diff910', 'src_diff920', 'src_diff930', 'src_diff940',\n",
    "       'src_diff950', 'src_diff960', 'src_diff970', 'src_diff980',\n",
    "       'dst_diff650', 'dst_diff660', 'dst_diff670', 'dst_diff680',\n",
    "       'dst_diff690', 'dst_diff700', 'dst_diff710', 'dst_diff720',\n",
    "       'dst_diff730', 'dst_diff740', 'dst_diff750', 'dst_diff760',\n",
    "       'dst_diff770', 'dst_diff780', 'dst_diff790', 'dst_diff800',\n",
    "       'dst_diff810', 'dst_diff820', 'dst_diff830', 'dst_diff840',\n",
    "       'dst_diff850', 'dst_diff860', 'dst_diff870', 'dst_diff880',\n",
    "       'dst_diff890', 'dst_diff900', 'dst_diff910', 'dst_diff920',\n",
    "       'dst_diff930', 'dst_diff940', 'dst_diff950', 'dst_diff960',\n",
    "       'dst_diff970', 'dst_diff980']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_hhb.iloc[:train.shape[0],:].drop(columns=drop_col)\n",
    "y_train = train.loc[:,\"hhb\"]\n",
    "X_test = df_hhb.iloc[train.shape[0]:,:].drop(columns=drop_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb#0.842541\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, train_cv, y, y_cv = train_test_split(X_train,y_train, test_size=0.15, random_state=22882)\n",
    "\n",
    "def lgbm_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=288, num_rounds=41419):\n",
    "\n",
    "    param = {}\n",
    "\n",
    "    param['boosting'] = 'dart'\n",
    "    \n",
    "    param['objective'] = 'regression'\n",
    "\n",
    "    param['learning_rate'] = 0.05\n",
    "\n",
    "    param['max_depth'] = 10\n",
    "\n",
    "    param['metric'] = 'mae'\n",
    "    \n",
    "    param['is_training_metric'] = True\n",
    "    \n",
    "    param['min_child_weight'] = 1\n",
    "\n",
    "    param['bagging_fraction'] = 0.8\n",
    "    \n",
    "    param['num_leaves'] = 128\n",
    "\n",
    "    param['feature_fraction'] = 0.8\n",
    "\n",
    "    param['bagging_freq'] = 6\n",
    "    \n",
    "    param['seed'] = seed_val\n",
    "    \n",
    "    param['min_split_gain'] = 0.01\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    test_ds = lgb.Dataset(test_X, label=test_y)\n",
    "\n",
    "    model = lgb.train(param, train_ds, num_rounds,test_ds, early_stopping_rounds=180)\n",
    "\n",
    "    return model\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "\n",
    "model = lgbm_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "\n",
    "lgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =  model.predict(X_test)\n",
    "submission['hhb'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  hbo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hbo2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hbo2 = df_hbo2.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm 모델을 돌리는데 dart를 사용 많은 양을 학습시킨 후 최소의 mae num_rounds를 찾아내 재학습 시행 이를 반복함.\n",
    "# 모델을 학습시키며 성능의 감소를 가져오는 피쳐를 찾아내 제거.\n",
    "\n",
    "X_train = df_hbo2.iloc[:train.shape[0],:].drop(columns=['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src','rho_src_mean',\n",
    "    '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst', 'src_diff650', 'src_diff660',\n",
    "       'src_diff670', 'src_diff680', 'src_diff690', 'src_diff700',\n",
    "       'src_diff710', 'src_diff720', 'src_diff730', 'src_diff740',\n",
    "       'src_diff750', 'src_diff760', 'src_diff770', 'src_diff780',\n",
    "       'src_diff790', 'src_diff800', 'src_diff810', 'src_diff820',\n",
    "       'src_diff830', 'src_diff840', 'src_diff850', 'src_diff860',\n",
    "       'src_diff870', 'src_diff880', 'src_diff890', 'src_diff900',\n",
    "       'src_diff910', 'src_diff920', 'src_diff930', 'src_diff940',\n",
    "       'src_diff950', 'src_diff960', 'src_diff970', 'src_diff980',\n",
    "       'dst_diff650', 'dst_diff660', 'dst_diff670', 'dst_diff680',\n",
    "       'dst_diff690', 'dst_diff700', 'dst_diff710', 'dst_diff720',\n",
    "       'dst_diff730', 'dst_diff740', 'dst_diff750', 'dst_diff760',\n",
    "       'dst_diff770', 'dst_diff780', 'dst_diff790', 'dst_diff800',\n",
    "       'dst_diff810', 'dst_diff820', 'dst_diff830', 'dst_diff840',\n",
    "       'dst_diff850', 'dst_diff860', 'dst_diff870', 'dst_diff880',\n",
    "       'dst_diff890', 'dst_diff900', 'dst_diff910', 'dst_diff920',\n",
    "       'dst_diff930', 'dst_diff940', 'dst_diff950', 'dst_diff960',\n",
    "       'dst_diff970', 'dst_diff980','src_sum','dst_sum','sum_rate'])\n",
    "y_train = train.loc[:,\"hbo2\"]\n",
    "X_test = df_hbo2.iloc[train.shape[0]:,:].drop(columns=['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src','rho_src_mean',\n",
    "        '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst', 'src_diff650', 'src_diff660',\n",
    "       'src_diff670', 'src_diff680', 'src_diff690', 'src_diff700',\n",
    "       'src_diff710', 'src_diff720', 'src_diff730', 'src_diff740',\n",
    "       'src_diff750', 'src_diff760', 'src_diff770', 'src_diff780',\n",
    "       'src_diff790', 'src_diff800', 'src_diff810', 'src_diff820',\n",
    "       'src_diff830', 'src_diff840', 'src_diff850', 'src_diff860',\n",
    "       'src_diff870', 'src_diff880', 'src_diff890', 'src_diff900',\n",
    "       'src_diff910', 'src_diff920', 'src_diff930', 'src_diff940',\n",
    "       'src_diff950', 'src_diff960', 'src_diff970', 'src_diff980',\n",
    "       'dst_diff650', 'dst_diff660', 'dst_diff670', 'dst_diff680',\n",
    "       'dst_diff690', 'dst_diff700', 'dst_diff710', 'dst_diff720',\n",
    "       'dst_diff730', 'dst_diff740', 'dst_diff750', 'dst_diff760',\n",
    "       'dst_diff770', 'dst_diff780', 'dst_diff790', 'dst_diff800',\n",
    "       'dst_diff810', 'dst_diff820', 'dst_diff830', 'dst_diff840',\n",
    "       'dst_diff850', 'dst_diff860', 'dst_diff870', 'dst_diff880',\n",
    "       'dst_diff890', 'dst_diff900', 'dst_diff910', 'dst_diff920',\n",
    "       'dst_diff930', 'dst_diff940', 'dst_diff950', 'dst_diff960',\n",
    "       'dst_diff970', 'dst_diff980','src_sum','dst_sum','sum_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb#0.217199\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, train_cv, y, y_cv = train_test_split(X_train,y_train, test_size=0.15, random_state=22882)\n",
    "\n",
    "def lgbm_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=288, num_rounds=21921):\n",
    "\n",
    "    param = {}\n",
    "\n",
    "    param['boosting'] = 'dart'\n",
    "    \n",
    "    param['objective'] = 'regression'\n",
    "\n",
    "    param['learning_rate'] = 0.05\n",
    "\n",
    "    param['max_depth'] = 10\n",
    "\n",
    "    param['metric'] = 'mae'\n",
    "    \n",
    "    param['is_training_metric'] = True\n",
    "    \n",
    "    param['min_child_weight'] = 1\n",
    "\n",
    "    param['bagging_fraction'] = 0.8\n",
    "    \n",
    "    param['num_leaves'] = 128\n",
    "\n",
    "    param['feature_fraction'] = 0.8\n",
    "\n",
    "    param['bagging_freq'] = 6\n",
    "    \n",
    "    param['seed'] = seed_val\n",
    "    \n",
    "    param['min_split_gain'] = 0.01\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    test_ds = lgb.Dataset(test_X, label=test_y)\n",
    "\n",
    "    model = lgb.train(param, train_ds, num_rounds,test_ds, early_stopping_rounds=180)\n",
    "\n",
    "    return model\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "\n",
    "model = lgbm_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "\n",
    "lgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =  model.predict(X_test)\n",
    "submission['hbo2'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca = df_ca.loc[:,:\"흡수계수_990\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(10,350,10) : \n",
    "    for i in range(650,1000-j,10) : \n",
    "        df_ca['흡수nextrat{}_{}'.format(i,j)] = df_ca['흡수계수_{}'.format(i+j)] / df_ca['흡수계수_{}'.format(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca = df_ca.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm 모델을 돌리는데 dart를 사용 많은 양을 학습시킨 후 최소의 mae num_rounds를 찾아내 재학습 시행 이를 반복함.\n",
    "# 모델을 학습시키며 성능의 감소를 가져오는 피쳐를 찾아내 제거.\n",
    "\n",
    "X_train = df_ca.iloc[:train.shape[0],:].drop(columns=['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src', 'src_diff650', 'src_diff660',\n",
    "       'src_diff670', 'src_diff680', 'src_diff690', 'src_diff700',\n",
    "       'src_diff710', 'src_diff720', 'src_diff730', 'src_diff740',\n",
    "       'src_diff750', 'src_diff760', 'src_diff770', 'src_diff780',\n",
    "       'src_diff790', 'src_diff800', 'src_diff810', 'src_diff820',\n",
    "       'src_diff830', 'src_diff840', 'src_diff850', 'src_diff860',\n",
    "       'src_diff870', 'src_diff880', 'src_diff890', 'src_diff900',\n",
    "       'src_diff910', 'src_diff920', 'src_diff930', 'src_diff940',\n",
    "       'src_diff950', 'src_diff960', 'src_diff970', 'src_diff980',\n",
    "       'dst_diff650', 'dst_diff660', 'dst_diff670', 'dst_diff680',\n",
    "       'dst_diff690', 'dst_diff700', 'dst_diff710', 'dst_diff720',\n",
    "       'dst_diff730', 'dst_diff740', 'dst_diff750', 'dst_diff760',\n",
    "       'dst_diff770', 'dst_diff780', 'dst_diff790', 'dst_diff800',\n",
    "       'dst_diff810', 'dst_diff820', 'dst_diff830', 'dst_diff840',\n",
    "       'dst_diff850', 'dst_diff860', 'dst_diff870', 'dst_diff880',\n",
    "       'dst_diff890', 'dst_diff900', 'dst_diff910', 'dst_diff920',\n",
    "       'dst_diff930', 'dst_diff940', 'dst_diff950', 'dst_diff960',\n",
    "       'dst_diff970', 'dst_diff980','rho_src_mean',\n",
    "        '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst','src_sum','dst_sum','sum_rate'])\n",
    "y_train = train.loc[:,\"ca\"]\n",
    "X_test = df_ca.iloc[train.shape[0]:,:].drop(columns=['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src', 'src_diff650', 'src_diff660',\n",
    "       'src_diff670', 'src_diff680', 'src_diff690', 'src_diff700',\n",
    "       'src_diff710', 'src_diff720', 'src_diff730', 'src_diff740',\n",
    "       'src_diff750', 'src_diff760', 'src_diff770', 'src_diff780',\n",
    "       'src_diff790', 'src_diff800', 'src_diff810', 'src_diff820',\n",
    "       'src_diff830', 'src_diff840', 'src_diff850', 'src_diff860',\n",
    "       'src_diff870', 'src_diff880', 'src_diff890', 'src_diff900',\n",
    "       'src_diff910', 'src_diff920', 'src_diff930', 'src_diff940',\n",
    "       'src_diff950', 'src_diff960', 'src_diff970', 'src_diff980',\n",
    "       'dst_diff650', 'dst_diff660', 'dst_diff670', 'dst_diff680',\n",
    "       'dst_diff690', 'dst_diff700', 'dst_diff710', 'dst_diff720',\n",
    "       'dst_diff730', 'dst_diff740', 'dst_diff750', 'dst_diff760',\n",
    "       'dst_diff770', 'dst_diff780', 'dst_diff790', 'dst_diff800',\n",
    "       'dst_diff810', 'dst_diff820', 'dst_diff830', 'dst_diff840',\n",
    "       'dst_diff850', 'dst_diff860', 'dst_diff870', 'dst_diff880',\n",
    "       'dst_diff890', 'dst_diff900', 'dst_diff910', 'dst_diff920',\n",
    "       'dst_diff930', 'dst_diff940', 'dst_diff950', 'dst_diff960',\n",
    "       'dst_diff970', 'dst_diff980','rho_src_mean',\n",
    "        '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst','src_sum','dst_sum','sum_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb#0.217199\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, train_cv, y, y_cv = train_test_split(X_train,y_train, test_size=0.15, random_state=22882)\n",
    "\n",
    "def lgbm_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=288, num_rounds=24330):\n",
    "\n",
    "    param = {}\n",
    "\n",
    "    param['boosting'] = 'dart'\n",
    "    \n",
    "    param['objective'] = 'regression'\n",
    "\n",
    "    param['learning_rate'] = 0.05\n",
    "\n",
    "    param['max_depth'] = 10\n",
    "\n",
    "    param['metric'] = 'mae'\n",
    "    \n",
    "    param['is_training_metric'] = True\n",
    "    \n",
    "    param['min_child_weight'] = 1\n",
    "\n",
    "    param['bagging_fraction'] = 0.8\n",
    "    \n",
    "    param['num_leaves'] = 128\n",
    "\n",
    "    param['feature_fraction'] = 0.8\n",
    "\n",
    "    param['bagging_freq'] = 6\n",
    "    \n",
    "    param['seed'] = seed_val\n",
    "    \n",
    "    param['min_split_gain'] = 0.01\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    test_ds = lgb.Dataset(test_X, label=test_y)\n",
    "\n",
    "    model = lgb.train(param, train_ds, num_rounds,test_ds, early_stopping_rounds=180)\n",
    "\n",
    "    return model\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "\n",
    "model = lgbm_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "\n",
    "lgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =  model.predict(X_test)\n",
    "submission['ca'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = df_na.loc[:,:\"흡수계수_990\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = df_na.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(10,350,10) : \n",
    "    for i in range(650,1000-j,10) : \n",
    "        df_na['흡수nextrat{}_{}'.format(i,j)] = df_na['흡수계수_{}'.format(i+j)] / df_na['흡수계수_{}'.format(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm 모델을 돌리는데 dart를 사용 많은 양을 학습시킨 후 최소의 mae num_rounds를 찾아내 재학습 시행 이를 반복함.\n",
    "# 모델을 학습시키며 성능의 감소를 가져오는 피쳐를 찾아내 제거.\n",
    "\n",
    "X_train = df_na.iloc[:train.shape[0],:].drop(columns=['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src','650_dst_fft_imag', '660_dst_fft_imag', '670_dst_fft_imag',\n",
    "       '680_dst_fft_imag', '690_dst_fft_imag', '700_dst_fft_imag',\n",
    "       '710_dst_fft_imag', '720_dst_fft_imag', '730_dst_fft_imag',\n",
    "       '740_dst_fft_imag', '750_dst_fft_imag', '760_dst_fft_imag',\n",
    "       '770_dst_fft_imag', '780_dst_fft_imag', '790_dst_fft_imag',\n",
    "       '800_dst_fft_imag', '810_dst_fft_imag', '820_dst_fft_imag',\n",
    "       '830_dst_fft_imag', '840_dst_fft_imag', '850_dst_fft_imag',\n",
    "       '860_dst_fft_imag', '870_dst_fft_imag', '880_dst_fft_imag',\n",
    "       '890_dst_fft_imag', '900_dst_fft_imag', '910_dst_fft_imag',\n",
    "       '920_dst_fft_imag', '930_dst_fft_imag', '940_dst_fft_imag',\n",
    "       '950_dst_fft_imag', '960_dst_fft_imag', '970_dst_fft_imag',\n",
    "       '980_dst_fft_imag', '990_dst_fft_imag','650_dst_fft_real', '660_dst_fft_real', '670_dst_fft_real',\n",
    "       '680_dst_fft_real', '690_dst_fft_real', '700_dst_fft_real',\n",
    "       '710_dst_fft_real', '720_dst_fft_real', '730_dst_fft_real',\n",
    "       '740_dst_fft_real', '750_dst_fft_real', '760_dst_fft_real',\n",
    "       '770_dst_fft_real', '780_dst_fft_real', '790_dst_fft_real',\n",
    "       '800_dst_fft_real', '810_dst_fft_real', '820_dst_fft_real',\n",
    "       '830_dst_fft_real', '840_dst_fft_real', '850_dst_fft_real',\n",
    "       '860_dst_fft_real', '870_dst_fft_real', '880_dst_fft_real',\n",
    "       '890_dst_fft_real', '900_dst_fft_real', '910_dst_fft_real',\n",
    "       '920_dst_fft_real', '930_dst_fft_real', '940_dst_fft_real',\n",
    "       '950_dst_fft_real', '960_dst_fft_real', '970_dst_fft_real',\n",
    "       '980_dst_fft_real', '990_dst_fft_real','src_diff650', 'src_diff660', 'src_diff670', 'src_diff680',\n",
    "       'src_diff690', 'src_diff700', 'src_diff710', 'src_diff720',\n",
    "       'src_diff730', 'src_diff740', 'src_diff750', 'src_diff760',\n",
    "       'src_diff770', 'src_diff780', 'src_diff790', 'src_diff800',\n",
    "       'src_diff810', 'src_diff820', 'src_diff830', 'src_diff840',\n",
    "       'src_diff850', 'src_diff860', 'src_diff870', 'src_diff880',\n",
    "       'src_diff890', 'src_diff900', 'src_diff910', 'src_diff920',\n",
    "       'src_diff930', 'src_diff940', 'src_diff950', 'src_diff960',\n",
    "       'src_diff970', 'src_diff980', 'dst_diff650', 'dst_diff660',\n",
    "       'dst_diff670', 'dst_diff680', 'dst_diff690', 'dst_diff700',\n",
    "       'dst_diff710', 'dst_diff720', 'dst_diff730', 'dst_diff740',\n",
    "       'dst_diff750', 'dst_diff760', 'dst_diff770', 'dst_diff780',\n",
    "       'dst_diff790', 'dst_diff800', 'dst_diff810', 'dst_diff820',\n",
    "       'dst_diff830', 'dst_diff840', 'dst_diff850', 'dst_diff860',\n",
    "       'dst_diff870', 'dst_diff880', 'dst_diff890', 'dst_diff900',\n",
    "       'dst_diff910', 'dst_diff920', 'dst_diff930', 'dst_diff940',\n",
    "       'dst_diff950', 'dst_diff960', 'dst_diff970', 'dst_diff980','rho_src_mean','rho_dst_mean',\n",
    "        '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst','src_sum','dst_sum','sum_rate'])\n",
    "y_train = train.loc[:,\"na\"]\n",
    "X_test = df_na.iloc[train.shape[0]:,:].drop(columns=['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src','650_dst_fft_imag', '660_dst_fft_imag', '670_dst_fft_imag',\n",
    "       '680_dst_fft_imag', '690_dst_fft_imag', '700_dst_fft_imag',\n",
    "       '710_dst_fft_imag', '720_dst_fft_imag', '730_dst_fft_imag',\n",
    "       '740_dst_fft_imag', '750_dst_fft_imag', '760_dst_fft_imag',\n",
    "       '770_dst_fft_imag', '780_dst_fft_imag', '790_dst_fft_imag',\n",
    "       '800_dst_fft_imag', '810_dst_fft_imag', '820_dst_fft_imag',\n",
    "       '830_dst_fft_imag', '840_dst_fft_imag', '850_dst_fft_imag',\n",
    "       '860_dst_fft_imag', '870_dst_fft_imag', '880_dst_fft_imag',\n",
    "       '890_dst_fft_imag', '900_dst_fft_imag', '910_dst_fft_imag',\n",
    "       '920_dst_fft_imag', '930_dst_fft_imag', '940_dst_fft_imag',\n",
    "       '950_dst_fft_imag', '960_dst_fft_imag', '970_dst_fft_imag',\n",
    "       '980_dst_fft_imag', '990_dst_fft_imag','650_dst_fft_real', '660_dst_fft_real', '670_dst_fft_real',\n",
    "       '680_dst_fft_real', '690_dst_fft_real', '700_dst_fft_real',\n",
    "       '710_dst_fft_real', '720_dst_fft_real', '730_dst_fft_real',\n",
    "       '740_dst_fft_real', '750_dst_fft_real', '760_dst_fft_real',\n",
    "       '770_dst_fft_real', '780_dst_fft_real', '790_dst_fft_real',\n",
    "       '800_dst_fft_real', '810_dst_fft_real', '820_dst_fft_real',\n",
    "       '830_dst_fft_real', '840_dst_fft_real', '850_dst_fft_real',\n",
    "       '860_dst_fft_real', '870_dst_fft_real', '880_dst_fft_real',\n",
    "       '890_dst_fft_real', '900_dst_fft_real', '910_dst_fft_real',\n",
    "       '920_dst_fft_real', '930_dst_fft_real', '940_dst_fft_real',\n",
    "       '950_dst_fft_real', '960_dst_fft_real', '970_dst_fft_real',\n",
    "       '980_dst_fft_real', '990_dst_fft_real','src_diff650', 'src_diff660', 'src_diff670', 'src_diff680',\n",
    "       'src_diff690', 'src_diff700', 'src_diff710', 'src_diff720',\n",
    "       'src_diff730', 'src_diff740', 'src_diff750', 'src_diff760',\n",
    "       'src_diff770', 'src_diff780', 'src_diff790', 'src_diff800',\n",
    "       'src_diff810', 'src_diff820', 'src_diff830', 'src_diff840',\n",
    "       'src_diff850', 'src_diff860', 'src_diff870', 'src_diff880',\n",
    "       'src_diff890', 'src_diff900', 'src_diff910', 'src_diff920',\n",
    "       'src_diff930', 'src_diff940', 'src_diff950', 'src_diff960',\n",
    "       'src_diff970', 'src_diff980', 'dst_diff650', 'dst_diff660',\n",
    "       'dst_diff670', 'dst_diff680', 'dst_diff690', 'dst_diff700',\n",
    "       'dst_diff710', 'dst_diff720', 'dst_diff730', 'dst_diff740',\n",
    "       'dst_diff750', 'dst_diff760', 'dst_diff770', 'dst_diff780',\n",
    "       'dst_diff790', 'dst_diff800', 'dst_diff810', 'dst_diff820',\n",
    "       'dst_diff830', 'dst_diff840', 'dst_diff850', 'dst_diff860',\n",
    "       'dst_diff870', 'dst_diff880', 'dst_diff890', 'dst_diff900',\n",
    "       'dst_diff910', 'dst_diff920', 'dst_diff930', 'dst_diff940',\n",
    "       'dst_diff950', 'dst_diff960', 'dst_diff970', 'dst_diff980','rho_src_mean','rho_dst_mean',\n",
    "                    '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst','src_sum','dst_sum','sum_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb#0.217199\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, train_cv, y, y_cv = train_test_split(X_train,y_train, test_size=0.15, random_state=22882)\n",
    "\n",
    "def lgbm_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=288, num_rounds=22076):\n",
    "\n",
    "    param = {}\n",
    "\n",
    "    param['boosting'] = 'dart'\n",
    "    \n",
    "    param['objective'] = 'regression'\n",
    "\n",
    "    param['learning_rate'] = 0.05\n",
    "\n",
    "    param['max_depth'] = 10\n",
    "\n",
    "    param['metric'] = 'mae'\n",
    "    \n",
    "    param['is_training_metric'] = True\n",
    "    \n",
    "    param['min_child_weight'] = 1\n",
    "\n",
    "    param['bagging_fraction'] = 0.8\n",
    "    \n",
    "    param['num_leaves'] = 128\n",
    "\n",
    "    param['feature_fraction'] = 0.8\n",
    "\n",
    "    param['bagging_freq'] = 6\n",
    "    \n",
    "    param['seed'] = seed_val\n",
    "    \n",
    "    param['min_split_gain'] = 0.01\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    test_ds = lgb.Dataset(test_X, label=test_y)\n",
    "\n",
    "    model = lgb.train(param, train_ds, num_rounds,test_ds, early_stopping_rounds=180)\n",
    "\n",
    "    return model\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "\n",
    "model = lgbm_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "\n",
    "lgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test =  model.predict(X_test)\n",
    "submission['na'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_zero(x) : \n",
    "    if x <= 0 : \n",
    "        return(0)\n",
    "    else : return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in submission.columns : \n",
    "    submission[i] = submission[i].apply(to_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('edit_dst22882.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CASE 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  hhb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hhb = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hhb = df_hhb.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm 모델을 돌리는데 dart를 사용 많은 양을 학습시킨 후 최소의 mae num_rounds를 찾아내 재학습 시행 이를 반복함.\n",
    "# 모델을 학습시키며 성능의 감소를 가져오는 피쳐를 찾아내 제거.\n",
    "\n",
    "drop_col = ['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src',\n",
    "        '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst',\n",
    "'960_dst','970_dst','980_dst','990_dst', 'src_diff650', 'src_diff660',\n",
    "       'src_diff670', 'src_diff680', 'src_diff690', 'src_diff700',\n",
    "       'src_diff710', 'src_diff720', 'src_diff730', 'src_diff740',\n",
    "       'src_diff750', 'src_diff760', 'src_diff770', 'src_diff780',\n",
    "       'src_diff790', 'src_diff800', 'src_diff810', 'src_diff820',\n",
    "       'src_diff830', 'src_diff840', 'src_diff850', 'src_diff860',\n",
    "       'src_diff870', 'src_diff880', 'src_diff890', 'src_diff900',\n",
    "       'src_diff910', 'src_diff920', 'src_diff930', 'src_diff940',\n",
    "       'src_diff950', 'src_diff960', 'src_diff970', 'src_diff980',\n",
    "       'dst_diff650', 'dst_diff660', 'dst_diff670', 'dst_diff680',\n",
    "       'dst_diff690', 'dst_diff700', 'dst_diff710', 'dst_diff720',\n",
    "       'dst_diff730', 'dst_diff740', 'dst_diff750', 'dst_diff760',\n",
    "       'dst_diff770', 'dst_diff780', 'dst_diff790', 'dst_diff800',\n",
    "       'dst_diff810', 'dst_diff820', 'dst_diff830', 'dst_diff840',\n",
    "       'dst_diff850', 'dst_diff860', 'dst_diff870', 'dst_diff880',\n",
    "       'dst_diff890', 'dst_diff900', 'dst_diff910', 'dst_diff920',\n",
    "       'dst_diff930', 'dst_diff940', 'dst_diff950', 'dst_diff960',\n",
    "       'dst_diff970', 'dst_diff980']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_hhb.iloc[:train.shape[0],:].drop(columns=drop_col)\n",
    "y_train = train.loc[:,\"hhb\"]\n",
    "X_test = df_hhb.iloc[train.shape[0]:,:].drop(columns=drop_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb#0.842541\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, train_cv, y, y_cv = train_test_split(X_train,y_train, test_size=0.15, random_state=3402)\n",
    "\n",
    "def lgbm_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=288, num_rounds=50000):\n",
    "\n",
    "    param = {}\n",
    "\n",
    "    param['boosting'] = 'dart'\n",
    "    \n",
    "    param['objective'] = 'regression'\n",
    "\n",
    "    param['learning_rate'] = 0.05\n",
    "\n",
    "    param['max_depth'] = 10\n",
    "\n",
    "    param['metric'] = 'mae'\n",
    "    \n",
    "    param['is_training_metric'] = True\n",
    "    \n",
    "    param['min_child_weight'] = 1\n",
    "\n",
    "    param['bagging_fraction'] = 0.8\n",
    "    \n",
    "    param['num_leaves'] = 128\n",
    "\n",
    "    param['feature_fraction'] = 0.8\n",
    "\n",
    "    param['bagging_freq'] = 6\n",
    "    \n",
    "    param['seed'] = seed_val\n",
    "    \n",
    "    param['min_split_gain'] = 0.01\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    test_ds = lgb.Dataset(test_X, label=test_y)\n",
    "\n",
    "    model = lgb.train(param, train_ds, num_rounds,test_ds, early_stopping_rounds=180)\n",
    "\n",
    "    return model\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "\n",
    "model = lgbm_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "\n",
    "lgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =  model.predict(X_test)\n",
    "submission['hhb'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  hbo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hbo2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hbo2 = df_hbo2.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm 모델을 돌리는데 dart를 사용 많은 양을 학습시킨 후 최소의 mae num_rounds를 찾아내 재학습 시행 이를 반복함.\n",
    "# 모델을 학습시키며 성능의 감소를 가져오는 피쳐를 찾아내 제거.\n",
    "\n",
    "X_train = df_hbo2.iloc[:train.shape[0],:].drop(columns=['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src','rho_src_mean',\n",
    "    '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst', 'src_diff650', 'src_diff660',\n",
    "       'src_diff670', 'src_diff680', 'src_diff690', 'src_diff700',\n",
    "       'src_diff710', 'src_diff720', 'src_diff730', 'src_diff740',\n",
    "       'src_diff750', 'src_diff760', 'src_diff770', 'src_diff780',\n",
    "       'src_diff790', 'src_diff800', 'src_diff810', 'src_diff820',\n",
    "       'src_diff830', 'src_diff840', 'src_diff850', 'src_diff860',\n",
    "       'src_diff870', 'src_diff880', 'src_diff890', 'src_diff900',\n",
    "       'src_diff910', 'src_diff920', 'src_diff930', 'src_diff940',\n",
    "       'src_diff950', 'src_diff960', 'src_diff970', 'src_diff980',\n",
    "       'dst_diff650', 'dst_diff660', 'dst_diff670', 'dst_diff680',\n",
    "       'dst_diff690', 'dst_diff700', 'dst_diff710', 'dst_diff720',\n",
    "       'dst_diff730', 'dst_diff740', 'dst_diff750', 'dst_diff760',\n",
    "       'dst_diff770', 'dst_diff780', 'dst_diff790', 'dst_diff800',\n",
    "       'dst_diff810', 'dst_diff820', 'dst_diff830', 'dst_diff840',\n",
    "       'dst_diff850', 'dst_diff860', 'dst_diff870', 'dst_diff880',\n",
    "       'dst_diff890', 'dst_diff900', 'dst_diff910', 'dst_diff920',\n",
    "       'dst_diff930', 'dst_diff940', 'dst_diff950', 'dst_diff960',\n",
    "       'dst_diff970', 'dst_diff980','src_sum','dst_sum','sum_rate'])\n",
    "y_train = train.loc[:,\"hbo2\"]\n",
    "X_test = df_hbo2.iloc[train.shape[0]:,:].drop(columns=['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src','rho_src_mean',\n",
    "        '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst', 'src_diff650', 'src_diff660',\n",
    "       'src_diff670', 'src_diff680', 'src_diff690', 'src_diff700',\n",
    "       'src_diff710', 'src_diff720', 'src_diff730', 'src_diff740',\n",
    "       'src_diff750', 'src_diff760', 'src_diff770', 'src_diff780',\n",
    "       'src_diff790', 'src_diff800', 'src_diff810', 'src_diff820',\n",
    "       'src_diff830', 'src_diff840', 'src_diff850', 'src_diff860',\n",
    "       'src_diff870', 'src_diff880', 'src_diff890', 'src_diff900',\n",
    "       'src_diff910', 'src_diff920', 'src_diff930', 'src_diff940',\n",
    "       'src_diff950', 'src_diff960', 'src_diff970', 'src_diff980',\n",
    "       'dst_diff650', 'dst_diff660', 'dst_diff670', 'dst_diff680',\n",
    "       'dst_diff690', 'dst_diff700', 'dst_diff710', 'dst_diff720',\n",
    "       'dst_diff730', 'dst_diff740', 'dst_diff750', 'dst_diff760',\n",
    "       'dst_diff770', 'dst_diff780', 'dst_diff790', 'dst_diff800',\n",
    "       'dst_diff810', 'dst_diff820', 'dst_diff830', 'dst_diff840',\n",
    "       'dst_diff850', 'dst_diff860', 'dst_diff870', 'dst_diff880',\n",
    "       'dst_diff890', 'dst_diff900', 'dst_diff910', 'dst_diff920',\n",
    "       'dst_diff930', 'dst_diff940', 'dst_diff950', 'dst_diff960',\n",
    "       'dst_diff970', 'dst_diff980','src_sum','dst_sum','sum_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb#0.217199\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, train_cv, y, y_cv = train_test_split(X_train,y_train, test_size=0.15, random_state=3402)\n",
    "\n",
    "def lgbm_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=288, num_rounds=22222):\n",
    "\n",
    "    param = {}\n",
    "\n",
    "    param['boosting'] = 'dart'\n",
    "    \n",
    "    param['objective'] = 'regression'\n",
    "\n",
    "    param['learning_rate'] = 0.05\n",
    "\n",
    "    param['max_depth'] = 10\n",
    "\n",
    "    param['metric'] = 'mae'\n",
    "    \n",
    "    param['is_training_metric'] = True\n",
    "    \n",
    "    param['min_child_weight'] = 1\n",
    "\n",
    "    param['bagging_fraction'] = 0.8\n",
    "    \n",
    "    param['num_leaves'] = 128\n",
    "\n",
    "    param['feature_fraction'] = 0.8\n",
    "\n",
    "    param['bagging_freq'] = 6\n",
    "    \n",
    "    param['seed'] = seed_val\n",
    "    \n",
    "    param['min_split_gain'] = 0.01\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    test_ds = lgb.Dataset(test_X, label=test_y)\n",
    "\n",
    "    model = lgb.train(param, train_ds, num_rounds,test_ds, early_stopping_rounds=180)\n",
    "\n",
    "    return model\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "\n",
    "model = lgbm_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "\n",
    "lgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =  model.predict(X_test)\n",
    "submission['hbo2'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca = df_ca.loc[:,:\"흡수계수_990\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(10,350,10) : \n",
    "    for i in range(650,1000-j,10) : \n",
    "        df_ca['흡수nextrat{}_{}'.format(i,j)] = df_ca['흡수계수_{}'.format(i+j)] / df_ca['흡수계수_{}'.format(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ca = df_ca.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm 모델을 돌리는데 dart를 사용 많은 양을 학습시킨 후 최소의 mae num_rounds를 찾아내 재학습 시행 이를 반복함.\n",
    "# 모델을 학습시키며 성능의 감소를 가져오는 피쳐를 찾아내 제거.\n",
    "\n",
    "X_train = df_ca.iloc[:train.shape[0],:].drop(columns=['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src', 'src_diff650', 'src_diff660',\n",
    "       'src_diff670', 'src_diff680', 'src_diff690', 'src_diff700',\n",
    "       'src_diff710', 'src_diff720', 'src_diff730', 'src_diff740',\n",
    "       'src_diff750', 'src_diff760', 'src_diff770', 'src_diff780',\n",
    "       'src_diff790', 'src_diff800', 'src_diff810', 'src_diff820',\n",
    "       'src_diff830', 'src_diff840', 'src_diff850', 'src_diff860',\n",
    "       'src_diff870', 'src_diff880', 'src_diff890', 'src_diff900',\n",
    "       'src_diff910', 'src_diff920', 'src_diff930', 'src_diff940',\n",
    "       'src_diff950', 'src_diff960', 'src_diff970', 'src_diff980',\n",
    "       'dst_diff650', 'dst_diff660', 'dst_diff670', 'dst_diff680',\n",
    "       'dst_diff690', 'dst_diff700', 'dst_diff710', 'dst_diff720',\n",
    "       'dst_diff730', 'dst_diff740', 'dst_diff750', 'dst_diff760',\n",
    "       'dst_diff770', 'dst_diff780', 'dst_diff790', 'dst_diff800',\n",
    "       'dst_diff810', 'dst_diff820', 'dst_diff830', 'dst_diff840',\n",
    "       'dst_diff850', 'dst_diff860', 'dst_diff870', 'dst_diff880',\n",
    "       'dst_diff890', 'dst_diff900', 'dst_diff910', 'dst_diff920',\n",
    "       'dst_diff930', 'dst_diff940', 'dst_diff950', 'dst_diff960',\n",
    "       'dst_diff970', 'dst_diff980','rho_src_mean',\n",
    "        '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst','src_sum','dst_sum','sum_rate'])\n",
    "y_train = train.loc[:,\"ca\"]\n",
    "X_test = df_ca.iloc[train.shape[0]:,:].drop(columns=['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src', 'src_diff650', 'src_diff660',\n",
    "       'src_diff670', 'src_diff680', 'src_diff690', 'src_diff700',\n",
    "       'src_diff710', 'src_diff720', 'src_diff730', 'src_diff740',\n",
    "       'src_diff750', 'src_diff760', 'src_diff770', 'src_diff780',\n",
    "       'src_diff790', 'src_diff800', 'src_diff810', 'src_diff820',\n",
    "       'src_diff830', 'src_diff840', 'src_diff850', 'src_diff860',\n",
    "       'src_diff870', 'src_diff880', 'src_diff890', 'src_diff900',\n",
    "       'src_diff910', 'src_diff920', 'src_diff930', 'src_diff940',\n",
    "       'src_diff950', 'src_diff960', 'src_diff970', 'src_diff980',\n",
    "       'dst_diff650', 'dst_diff660', 'dst_diff670', 'dst_diff680',\n",
    "       'dst_diff690', 'dst_diff700', 'dst_diff710', 'dst_diff720',\n",
    "       'dst_diff730', 'dst_diff740', 'dst_diff750', 'dst_diff760',\n",
    "       'dst_diff770', 'dst_diff780', 'dst_diff790', 'dst_diff800',\n",
    "       'dst_diff810', 'dst_diff820', 'dst_diff830', 'dst_diff840',\n",
    "       'dst_diff850', 'dst_diff860', 'dst_diff870', 'dst_diff880',\n",
    "       'dst_diff890', 'dst_diff900', 'dst_diff910', 'dst_diff920',\n",
    "       'dst_diff930', 'dst_diff940', 'dst_diff950', 'dst_diff960',\n",
    "       'dst_diff970', 'dst_diff980','rho_src_mean',\n",
    "        '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst','src_sum','dst_sum','sum_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb#0.217199\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, train_cv, y, y_cv = train_test_split(X_train,y_train, test_size=0.15, random_state=3402)\n",
    "\n",
    "def lgbm_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=288, num_rounds=45530):\n",
    "\n",
    "    param = {}\n",
    "\n",
    "    param['boosting'] = 'dart'\n",
    "    \n",
    "    param['objective'] = 'regression'\n",
    "\n",
    "    param['learning_rate'] = 0.05\n",
    "\n",
    "    param['max_depth'] = 10\n",
    "\n",
    "    param['metric'] = 'mae'\n",
    "    \n",
    "    param['is_training_metric'] = True\n",
    "    \n",
    "    param['min_child_weight'] = 1\n",
    "\n",
    "    param['bagging_fraction'] = 0.8\n",
    "    \n",
    "    param['num_leaves'] = 128\n",
    "\n",
    "    param['feature_fraction'] = 0.8\n",
    "\n",
    "    param['bagging_freq'] = 6\n",
    "    \n",
    "    param['seed'] = seed_val\n",
    "    \n",
    "    param['min_split_gain'] = 0.01\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    test_ds = lgb.Dataset(test_X, label=test_y)\n",
    "\n",
    "    model = lgb.train(param, train_ds, num_rounds,test_ds, early_stopping_rounds=180)\n",
    "\n",
    "    return model\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "\n",
    "model = lgbm_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "\n",
    "lgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =  model.predict(X_test)\n",
    "submission['ca'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = df_na.loc[:,:\"흡수계수_990\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_na = df_na.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(10,350,10) : \n",
    "    for i in range(650,1000-j,10) : \n",
    "        df_na['흡수nextrat{}_{}'.format(i,j)] = df_na['흡수계수_{}'.format(i+j)] / df_na['흡수계수_{}'.format(i)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm 모델을 돌리는데 dart를 사용 많은 양을 학습시킨 후 최소의 mae num_rounds를 찾아내 재학습 시행 이를 반복함.\n",
    "# 모델을 학습시키며 성능의 감소를 가져오는 피쳐를 찾아내 제거.\n",
    "\n",
    "X_train = df_na.iloc[:train.shape[0],:].drop(columns=['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src','650_dst_fft_imag', '660_dst_fft_imag', '670_dst_fft_imag',\n",
    "       '680_dst_fft_imag', '690_dst_fft_imag', '700_dst_fft_imag',\n",
    "       '710_dst_fft_imag', '720_dst_fft_imag', '730_dst_fft_imag',\n",
    "       '740_dst_fft_imag', '750_dst_fft_imag', '760_dst_fft_imag',\n",
    "       '770_dst_fft_imag', '780_dst_fft_imag', '790_dst_fft_imag',\n",
    "       '800_dst_fft_imag', '810_dst_fft_imag', '820_dst_fft_imag',\n",
    "       '830_dst_fft_imag', '840_dst_fft_imag', '850_dst_fft_imag',\n",
    "       '860_dst_fft_imag', '870_dst_fft_imag', '880_dst_fft_imag',\n",
    "       '890_dst_fft_imag', '900_dst_fft_imag', '910_dst_fft_imag',\n",
    "       '920_dst_fft_imag', '930_dst_fft_imag', '940_dst_fft_imag',\n",
    "       '950_dst_fft_imag', '960_dst_fft_imag', '970_dst_fft_imag',\n",
    "       '980_dst_fft_imag', '990_dst_fft_imag','650_dst_fft_real', '660_dst_fft_real', '670_dst_fft_real',\n",
    "       '680_dst_fft_real', '690_dst_fft_real', '700_dst_fft_real',\n",
    "       '710_dst_fft_real', '720_dst_fft_real', '730_dst_fft_real',\n",
    "       '740_dst_fft_real', '750_dst_fft_real', '760_dst_fft_real',\n",
    "       '770_dst_fft_real', '780_dst_fft_real', '790_dst_fft_real',\n",
    "       '800_dst_fft_real', '810_dst_fft_real', '820_dst_fft_real',\n",
    "       '830_dst_fft_real', '840_dst_fft_real', '850_dst_fft_real',\n",
    "       '860_dst_fft_real', '870_dst_fft_real', '880_dst_fft_real',\n",
    "       '890_dst_fft_real', '900_dst_fft_real', '910_dst_fft_real',\n",
    "       '920_dst_fft_real', '930_dst_fft_real', '940_dst_fft_real',\n",
    "       '950_dst_fft_real', '960_dst_fft_real', '970_dst_fft_real',\n",
    "       '980_dst_fft_real', '990_dst_fft_real','src_diff650', 'src_diff660', 'src_diff670', 'src_diff680',\n",
    "       'src_diff690', 'src_diff700', 'src_diff710', 'src_diff720',\n",
    "       'src_diff730', 'src_diff740', 'src_diff750', 'src_diff760',\n",
    "       'src_diff770', 'src_diff780', 'src_diff790', 'src_diff800',\n",
    "       'src_diff810', 'src_diff820', 'src_diff830', 'src_diff840',\n",
    "       'src_diff850', 'src_diff860', 'src_diff870', 'src_diff880',\n",
    "       'src_diff890', 'src_diff900', 'src_diff910', 'src_diff920',\n",
    "       'src_diff930', 'src_diff940', 'src_diff950', 'src_diff960',\n",
    "       'src_diff970', 'src_diff980', 'dst_diff650', 'dst_diff660',\n",
    "       'dst_diff670', 'dst_diff680', 'dst_diff690', 'dst_diff700',\n",
    "       'dst_diff710', 'dst_diff720', 'dst_diff730', 'dst_diff740',\n",
    "       'dst_diff750', 'dst_diff760', 'dst_diff770', 'dst_diff780',\n",
    "       'dst_diff790', 'dst_diff800', 'dst_diff810', 'dst_diff820',\n",
    "       'dst_diff830', 'dst_diff840', 'dst_diff850', 'dst_diff860',\n",
    "       'dst_diff870', 'dst_diff880', 'dst_diff890', 'dst_diff900',\n",
    "       'dst_diff910', 'dst_diff920', 'dst_diff930', 'dst_diff940',\n",
    "       'dst_diff950', 'dst_diff960', 'dst_diff970', 'dst_diff980','rho_src_mean','rho_dst_mean',\n",
    "        '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst','src_sum','dst_sum','sum_rate'])\n",
    "y_train = train.loc[:,\"na\"]\n",
    "X_test = df_na.iloc[train.shape[0]:,:].drop(columns=['id','650_src', '660_src', '670_src', '680_src', '690_src', '700_src',\n",
    "       '710_src', '720_src', '730_src', '740_src', '750_src', '760_src',\n",
    "       '770_src', '780_src', '790_src', '800_src', '810_src', '820_src',\n",
    "       '830_src', '840_src', '850_src', '860_src', '870_src', '880_src',\n",
    "       '890_src', '900_src', '910_src', '920_src', '930_src', '940_src',\n",
    "       '950_src', '960_src', '970_src', '980_src', '990_src','650_dst_fft_imag', '660_dst_fft_imag', '670_dst_fft_imag',\n",
    "       '680_dst_fft_imag', '690_dst_fft_imag', '700_dst_fft_imag',\n",
    "       '710_dst_fft_imag', '720_dst_fft_imag', '730_dst_fft_imag',\n",
    "       '740_dst_fft_imag', '750_dst_fft_imag', '760_dst_fft_imag',\n",
    "       '770_dst_fft_imag', '780_dst_fft_imag', '790_dst_fft_imag',\n",
    "       '800_dst_fft_imag', '810_dst_fft_imag', '820_dst_fft_imag',\n",
    "       '830_dst_fft_imag', '840_dst_fft_imag', '850_dst_fft_imag',\n",
    "       '860_dst_fft_imag', '870_dst_fft_imag', '880_dst_fft_imag',\n",
    "       '890_dst_fft_imag', '900_dst_fft_imag', '910_dst_fft_imag',\n",
    "       '920_dst_fft_imag', '930_dst_fft_imag', '940_dst_fft_imag',\n",
    "       '950_dst_fft_imag', '960_dst_fft_imag', '970_dst_fft_imag',\n",
    "       '980_dst_fft_imag', '990_dst_fft_imag','650_dst_fft_real', '660_dst_fft_real', '670_dst_fft_real',\n",
    "       '680_dst_fft_real', '690_dst_fft_real', '700_dst_fft_real',\n",
    "       '710_dst_fft_real', '720_dst_fft_real', '730_dst_fft_real',\n",
    "       '740_dst_fft_real', '750_dst_fft_real', '760_dst_fft_real',\n",
    "       '770_dst_fft_real', '780_dst_fft_real', '790_dst_fft_real',\n",
    "       '800_dst_fft_real', '810_dst_fft_real', '820_dst_fft_real',\n",
    "       '830_dst_fft_real', '840_dst_fft_real', '850_dst_fft_real',\n",
    "       '860_dst_fft_real', '870_dst_fft_real', '880_dst_fft_real',\n",
    "       '890_dst_fft_real', '900_dst_fft_real', '910_dst_fft_real',\n",
    "       '920_dst_fft_real', '930_dst_fft_real', '940_dst_fft_real',\n",
    "       '950_dst_fft_real', '960_dst_fft_real', '970_dst_fft_real',\n",
    "       '980_dst_fft_real', '990_dst_fft_real','src_diff650', 'src_diff660', 'src_diff670', 'src_diff680',\n",
    "       'src_diff690', 'src_diff700', 'src_diff710', 'src_diff720',\n",
    "       'src_diff730', 'src_diff740', 'src_diff750', 'src_diff760',\n",
    "       'src_diff770', 'src_diff780', 'src_diff790', 'src_diff800',\n",
    "       'src_diff810', 'src_diff820', 'src_diff830', 'src_diff840',\n",
    "       'src_diff850', 'src_diff860', 'src_diff870', 'src_diff880',\n",
    "       'src_diff890', 'src_diff900', 'src_diff910', 'src_diff920',\n",
    "       'src_diff930', 'src_diff940', 'src_diff950', 'src_diff960',\n",
    "       'src_diff970', 'src_diff980', 'dst_diff650', 'dst_diff660',\n",
    "       'dst_diff670', 'dst_diff680', 'dst_diff690', 'dst_diff700',\n",
    "       'dst_diff710', 'dst_diff720', 'dst_diff730', 'dst_diff740',\n",
    "       'dst_diff750', 'dst_diff760', 'dst_diff770', 'dst_diff780',\n",
    "       'dst_diff790', 'dst_diff800', 'dst_diff810', 'dst_diff820',\n",
    "       'dst_diff830', 'dst_diff840', 'dst_diff850', 'dst_diff860',\n",
    "       'dst_diff870', 'dst_diff880', 'dst_diff890', 'dst_diff900',\n",
    "       'dst_diff910', 'dst_diff920', 'dst_diff930', 'dst_diff940',\n",
    "       'dst_diff950', 'dst_diff960', 'dst_diff970', 'dst_diff980','rho_src_mean','rho_dst_mean',\n",
    "                    '650_dst','660_dst','670_dst','680_dst','690_dst','700_dst','710_dst','720_dst',\n",
    "'730_dst','740_dst','750_dst','760_dst','770_dst','780_dst','790_dst', '800_dst',\n",
    "'810_dst', '820_dst','830_dst','840_dst','850_dst','860_dst','870_dst','880_dst','890_dst',\n",
    "'900_dst','910_dst','920_dst','930_dst','940_dst','950_dst','960_dst','970_dst','980_dst','990_dst','src_sum','dst_sum','sum_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb#0.217199\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_x, train_cv, y, y_cv = train_test_split(X_train,y_train, test_size=0.15, random_state=3402)\n",
    "\n",
    "def lgbm_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=288, num_rounds=21000):\n",
    "\n",
    "    param = {}\n",
    "\n",
    "    param['boosting'] = 'dart'\n",
    "    \n",
    "    param['objective'] = 'regression'\n",
    "\n",
    "    param['learning_rate'] = 0.05\n",
    "\n",
    "    param['max_depth'] = 10\n",
    "\n",
    "    param['metric'] = 'mae'\n",
    "    \n",
    "    param['is_training_metric'] = True\n",
    "    \n",
    "    param['min_child_weight'] = 1\n",
    "\n",
    "    param['bagging_fraction'] = 0.8\n",
    "    \n",
    "    param['num_leaves'] = 128\n",
    "\n",
    "    param['feature_fraction'] = 0.8\n",
    "\n",
    "    param['bagging_freq'] = 6\n",
    "    \n",
    "    param['seed'] = seed_val\n",
    "    \n",
    "    param['min_split_gain'] = 0.01\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    test_ds = lgb.Dataset(test_X, label=test_y)\n",
    "\n",
    "    model = lgb.train(param, train_ds, num_rounds,test_ds, early_stopping_rounds=180)\n",
    "\n",
    "    return model\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "\n",
    "model = lgbm_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)\n",
    "\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,18))\n",
    "\n",
    "lgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test =  model.predict(X_test)\n",
    "submission['na'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_zero(x) : \n",
    "    if x <= 0 : \n",
    "        return(0)\n",
    "    else : return(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in submission.columns : \n",
    "    submission[i] = submission[i].apply(to_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('edit_dst_3402.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CASE 5\n",
    "\n",
    "### 필요한 패키지 import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최종 제출 형태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('data/sample_submission.csv',engine='python')\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test.csv',engine='python').iloc[:,1:] # index행 제거하기\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv',engine='python').iloc[:,1:] # index행 제거하기\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터에 존재하는 예측하고자 하는 값 추출 및 제거\n",
    "train_y = train.iloc[:,-4:]\n",
    "train = train.iloc[:,:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train data에 존재하는 Null값 확인\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dacon에 있는 결측치 처리방법 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dst data에만 Null값이 있는것을 확인하여 dst만 추출\n",
    "train_dst = train.filter(regex='_dst$', axis=1).replace(0, np.NaN) # dst 데이터만 따로 뺀다.\n",
    "test_dst = test.filter(regex='_dst$', axis=1).replace(0, np.NaN) # 보간을 하기위해 결측값을 삭제한다.\n",
    "test_dst.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 보간법 방법중 linear 방법을 활용하여 NaN값 삽입 \n",
    "train_dst = train_dst.interpolate(methods='linear', axis=1)\n",
    "test_dst = test_dst.interpolate(methods='linear', axis=1)\n",
    "# 스팩트럼 데이터에서 연속해서 NaN이 있는 경우 처리가 안되기 때문에 이러한 값은 전부 0으로 처리\n",
    "train_dst.fillna(0, inplace=True) \n",
    "test_dst.fillna(0, inplace=True)\n",
    "test_dst.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.update(train_dst) # 보간한 데이터를 기존 데이터프레임에 업데이트 한다.\n",
    "test.update(test_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 다 채워진 모습을 볼 수 있다.\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src와 dst간 사칙연산 결과 삽입. 어느정도 성능의 향상 존재 + dst가 0인 경우가 있기 때문에 1e-18 추가해서 계산.\n",
    "\n",
    "add_list=['650_add', '660_add', '670_add', '680_add', '690_add', '700_add', '710_add', '720_add', '730_add', \n",
    "          '740_add', '750_add', '760_add', '770_add', '780_add', '790_add', '800_add', '810_add', '820_add', \n",
    "          '830_add', '840_add', '850_add', '860_add', '870_add', '880_add', '890_add', '900_add', '910_add', \n",
    "          '920_add', '930_add', '940_add', '950_add', '960_add', '970_add', '980_add', '990_add']\n",
    "diff_list=['650_diff', '660_diff', '670_diff', '680_diff', '690_diff', '700_diff', '710_diff', '720_diff', '730_diff', \n",
    "          '740_diff', '750_diff', '760_diff', '770_diff', '780_diff', '790_diff', '800_diff', '810_diff', '820_diff', \n",
    "          '830_diff', '840_diff', '850_diff', '860_diff', '870_diff', '880_diff', '890_diff', '900_diff', '910_diff', \n",
    "          '920_diff', '930_diff', '940_diff', '950_diff', '960_diff', '970_diff', '980_diff', '990_diff']\n",
    "div_list=['650_div', '660_div', '670_div', '680_div', '690_div', '700_div', '710_div', '720_div', '730_div', \n",
    "          '740_div', '750_div', '760_div', '770_div', '780_div', '790_div', '800_div', '810_div', '820_div', \n",
    "          '830_div', '840_div', '850_div', '860_div', '870_div', '880_div', '890_div', '900_div', '910_div', \n",
    "          '920_div', '930_div', '940_div', '950_div', '960_div', '970_div', '980_div', '990_div']\n",
    "multi_list=['650_multi', '660_multi', '670_multi', '680_multi', '690_multi', '700_multi', '710_multi', '720_multi', '730_multi', \n",
    "          '740_multi', '750_multi', '760_multi', '770_multi', '780_multi', '790_multi', '800_multi', '810_multi', '820_multi', \n",
    "          '830_multi', '840_multi', '850_multi', '860_multi', '870_multi', '880_multi', '890_multi', '900_multi', '910_multi', \n",
    "          '920_multi', '930_multi', '940_multi', '950_multi', '960_multi', '970_multi', '980_multi', '990_multi']\n",
    "dst_src_add = pd.DataFrame(train.iloc[:,1:36].values + train.iloc[:,36:71].values,columns=add_list)\n",
    "dst_src_diff = pd.DataFrame(train.iloc[:,1:36].values - train.iloc[:,36:71].values,columns=diff_list)\n",
    "dst_src_div = pd.DataFrame(train.iloc[:,1:36].values / (train.iloc[:,36:71].values+1e-18),columns=div_list)\n",
    "dst_src_mul = pd.DataFrame(train.iloc[:,1:36].values * train.iloc[:,36:71].values,columns=multi_list)\n",
    "train = pd.concat([train,dst_src_add],axis=1)\n",
    "train = pd.concat([train,dst_src_diff],axis=1)\n",
    "train = pd.concat([train,dst_src_div],axis=1)\n",
    "train = pd.concat([train,dst_src_mul],axis=1)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_src_add = pd.DataFrame(test.iloc[:,1:36].values + test.iloc[:,36:71].values,columns=add_list)\n",
    "dst_src_diff = pd.DataFrame(test.iloc[:,1:36].values - test.iloc[:,36:71].values,columns=diff_list)\n",
    "dst_src_mul = pd.DataFrame(test.iloc[:,1:36].values * test.iloc[:,36:71].values,columns=multi_list)\n",
    "dst_src_div = pd.DataFrame(test.iloc[:,1:36].values / (test.iloc[:,36:71].values+1e-16),columns=div_list)\n",
    "test = pd.concat([test,dst_src_add],axis=1)\n",
    "test = pd.concat([test,dst_src_diff],axis=1)\n",
    "test = pd.concat([test,dst_src_div],axis=1)\n",
    "test = pd.concat([test,dst_src_mul],axis=1)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 퓨리에 변환 실시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_list=['650_src', '660_src', '670_src', '680_src', '690_src', '700_src', '710_src', '720_src', '730_src', \n",
    "          '740_src', '750_src', '760_src', '770_src', '780_src', '790_src', '800_src', '810_src', '820_src', \n",
    "          '830_src', '840_src', '850_src', '860_src', '870_src', '880_src', '890_src', '900_src', '910_src', \n",
    "          '920_src', '930_src', '940_src', '950_src', '960_src', '970_src', '980_src', '990_src']\n",
    "\n",
    "dst_list=['650_dst', '660_dst', '670_dst', '680_dst', '690_dst', '700_dst', '710_dst', '720_dst', '730_dst', \n",
    "          '740_dst', '750_dst', '760_dst', '770_dst', '780_dst', '790_dst', '800_dst', '810_dst', '820_dst', \n",
    "          '830_dst', '840_dst', '850_dst', '860_dst', '870_dst', '880_dst', '890_dst', '900_dst', '910_dst', \n",
    "          '920_dst', '930_dst', '940_dst', '950_dst', '960_dst', '970_dst', '980_dst', '990_dst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha_real=train[dst_list]\n",
    "alpha_imag=train[dst_list]\n",
    "\n",
    "beta_real=test[dst_list]\n",
    "beta_imag=test[dst_list]\n",
    "\n",
    "for i in tqdm(alpha_real.index):\n",
    "    alpha_real.loc[i]=alpha_real.loc[i] - alpha_real.loc[i].mean()\n",
    "    alpha_imag.loc[i]=alpha_imag.loc[i] - alpha_real.loc[i].mean()\n",
    "    \n",
    "    alpha_real.loc[i] = np.fft.fft(alpha_real.loc[i], norm='ortho').real\n",
    "    alpha_imag.loc[i] = np.fft.fft(alpha_imag.loc[i], norm='ortho').imag\n",
    "\n",
    "    \n",
    "for i in tqdm(beta_real.index):\n",
    "    beta_real.loc[i]=beta_real.loc[i] - beta_real.loc[i].mean()\n",
    "    beta_imag.loc[i]=beta_imag.loc[i] - beta_imag.loc[i].mean()\n",
    "    \n",
    "    beta_real.loc[i] = np.fft.fft(beta_real.loc[i], norm='ortho').real\n",
    "    beta_imag.loc[i] = np.fft.fft(beta_imag.loc[i], norm='ortho').imag\n",
    "    \n",
    "real_part=[]\n",
    "imag_part=[]\n",
    "\n",
    "for col in dst_list:\n",
    "    real_part.append(col + '_fft_real')\n",
    "    imag_part.append(col + '_fft_imag')\n",
    "    \n",
    "alpha_real.columns=real_part\n",
    "alpha_imag.columns=imag_part\n",
    "alpha = pd.concat((alpha_real, alpha_imag), axis=1)\n",
    "\n",
    "beta_real.columns=real_part\n",
    "beta_imag.columns=imag_part\n",
    "beta=pd.concat((beta_real, beta_imag), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.concat((train, alpha), axis=1)\n",
    "test=pd.concat((test, beta), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 흡광계수 만들기 (빛의 흡수율 활용)\n",
    "\n",
    "src는 광원의 빛의 세기 dst는 측정된 빛의 세기 활용하여 흡광계수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(650,1000,10) : \n",
    "    train['흡수계수_{}'.format(i)] = np.log10(train['{}_src'.format(i)] / train['{}_dst'.format(i)]) / train.rho\n",
    "#     train['투과율_{}'.format(i)] = np.log10(train['{}_src'.format(i)] / train['{}_dst'.format(i)])\n",
    "    test['흡수계수_{}'.format(i)] = np.log10(test['{}_src'.format(i)] / test['{}_dst'.format(i)]) / test.rho\n",
    "#     test['투과율_{}'.format(i)] = np.log10(test['{}_src'.format(i)] / test['{}_dst'.format(i)])\n",
    "    \n",
    "train = train.fillna(0)\n",
    "test = test.fillna(0)\n",
    "\n",
    "train.isnull().sum().sum(), test.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 흡수량 만들어 삽입\n",
    "\n",
    "src의 실제 세기 대비 흡수된 빛의 세기를 계산하여 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srrho_lisrho = ['65_srrho', '66_srrho', '67_srrho', '68_srrho', '69_srrho', '70_srrho', '71_srrho', '72_srrho', '73_srrho', \n",
    "          '74_srrho', '75_srrho', '76_srrho', '77_srrho', '78_srrho', '79_srrho', '80_srrho', '81_srrho', '82_srrho', \n",
    "          '83_srrho', '84_srrho', '85_srrho', '86_srrho', '87_srrho', '88_srrho', '89_srrho', '90_srrho', '91_srrho', \n",
    "          '92_srrho', '93_srrho', '94_srrho', '95_srrho', '96_srrho', '97_srrho', '98_srrho', '99_srrho','65_dsrho',\n",
    "          '66_dsrho', '67_dsrho', '68_dsrho', '69_dsrho', '70_dsrho', '71_dsrho', '72_dsrho', '73_dsrho', \n",
    "          '74_dsrho', '75_dsrho', '76_dsrho', '77_dsrho', '78_dsrho', '79_dsrho', '80_dsrho', '81_dsrho', '82_dsrho', \n",
    "          '83_dsrho', '84_dsrho', '85_dsrho', '86_dsrho', '87_dsrho', '88_dsrho', '89_dsrho', '90_dsrho', '91_dsrho', \n",
    "          '92_dsrho', '93_dsrho', '94_dsrho', '95_dsrho', '96_dsrho', '97_dsrho', '98_dsrho', '99_dsrho']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(650,1000,10) : \n",
    "    train['흡수량_{}'.format(i)] = (train['{}_src'.format(i)]-train['{}_dst'.format(i)])/(train['{}_src'.format(i)]+1e-20)\n",
    "    test['흡수량_{}'.format(i)] =  (test['{}_src'.format(i)]-test['{}_dst'.format(i)])/(test['{}_src'.format(i)]+1e-20)\n",
    "a = train.iloc[:,1:71].apply(lambda x: x/(train['rho']^2))\n",
    "b = test.iloc[:,1:71].apply(lambda x: x/(test['rho']^2))\n",
    "a.columns = srrho_lisrho\n",
    "b.columns = srrho_lisrho\n",
    "train = pd.concat([train,a],axis=1)\n",
    "test = pd.concat([test,b],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이렇게 구한 흡수계수 활용 흡수평균 및 편차 합 변수 만듦\n",
    "train['흡수평균'] = train.iloc[:,281:316].replace(np.inf,1).replace(-np.inf,-1).mean(axis=1)\n",
    "train['흡수편차'] = train.iloc[:,281:316].replace(np.inf,1).replace(-np.inf,-1).std(axis=1)\n",
    "train['흡수합'] = train.iloc[:,281:316].replace(np.inf,1).replace(-np.inf,-1).sum(axis=1)\n",
    "test['흡수평균'] = test.iloc[:,281:316].replace(np.inf,1).replace(-np.inf,-1).mean(axis=1)\n",
    "test['흡수편차'] = test.iloc[:,281:316].replace(np.inf,1).replace(-np.inf,-1).std(axis=1)\n",
    "test['흡수합'] = test.iloc[:,281:316].replace(np.inf,1).replace(-np.inf,-1).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 흡수계수, src, dst의 값을 슬라이딩 하며 비율을 계산함 높은 성능 향상을 보임 \n",
    "for j in range(10,350,10) : \n",
    "    for i in range(650,1000-j,10) : \n",
    "        train['흡수nextrat{}_{}'.format(i,j)] = train['흡수계수_{}'.format(i+j)].replace(np.inf,1).replace(-np.inf,-1) / (train['흡수계수_{}'.format(i)].replace(np.inf,1).replace(-np.inf,-1)+1e-20)\n",
    "        train['srcnextrat{}_{}'.format(i,j)] = train['{}_src'.format(i+j)].replace(np.inf,1).replace(-np.inf,-1) / (train['{}_src'.format(i)].replace(np.inf,1).replace(-np.inf,-1)+1e-20)\n",
    "        train['dstnextrat{}_{}'.format(i,j)] = train['{}_dst'.format(i+j)].replace(np.inf,1).replace(-np.inf,-1) / (train['{}_dst'.format(i)].replace(np.inf,1).replace(-np.inf,-1)+1e-20)\n",
    "        test['흡수nextrat{}_{}'.format(i,j)] = test['흡수계수_{}'.format(i+j)].replace(np.inf,1).replace(-np.inf,-1) / (test['흡수계수_{}'.format(i)].replace(np.inf,1).replace(-np.inf,-1)+1e-20)\n",
    "        test['srcnextrat{}_{}'.format(i,j)] = test['{}_src'.format(i+j)].replace(np.inf,1).replace(-np.inf,-1) / (test['{}_src'.format(i)].replace(np.inf,1).replace(-np.inf,-1)+1e-20)\n",
    "        test['dstnextrat{}_{}'.format(i,j)] = test['{}_dst'.format(i+j)].replace(np.inf,1).replace(-np.inf,-1) / (test['{}_dst'.format(i)].replace(np.inf,1).replace(-np.inf,-1)+1e-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srr_liss=['65srr', '66srr', '67srr', '68srr', '69srr', '70srr', '71srr', '72srr', '73srr', \n",
    "          '74srr', '75srr', '76srr', '77srr', '78srr', '79srr', '80srr', '81srr', '82srr', \n",
    "          '83srr', '84srr', '85srr', '86srr', '87srr', '88srr', '89srr', '90srr', '91srr', \n",
    "          '92srr', '93srr', '94srr', '95srr', '96srr', '97srr', '98srr']\n",
    "dsr_liss = ['65dss','66dss', '67dss', '68dss', '69dss', '70dss', '71dss', '72dss', '73dss', \n",
    "          '74dss', '75dss', '76dss', '77dss', '78dss', '79dss', '80dss', '81dss', '82dss', \n",
    "          '83dss', '84dss', '85dss', '86dss', '87dss', '88dss', '89dss', '90dss', '91dss', \n",
    "          '92dss', '93dss', '94dss', '95dss', '96dss', '97dss', '98dss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한칸 뒤에 있는 값을 나누어 계산함 분모가 0일때를 대비하여 잔차 삽입\n",
    "a = pd.DataFrame(columns = srr_liss, index=range(10000))\n",
    "b = pd.DataFrame(columns = srr_liss, index=range(10000))\n",
    "c = pd.DataFrame(columns = dsr_liss, index=range(10000))\n",
    "d = pd.DataFrame(columns = dsr_liss, index=range(10000))\n",
    "for i in range(1,35):\n",
    "    a.iloc[:,i-1] = train.iloc[:,i]/(train.iloc[:,i+1]+1e-20)\n",
    "    b.iloc[:,i-1] = test.iloc[:,i]/(test.iloc[:,i+1]+1e-20)\n",
    "    c.iloc[:,i-1] = train.iloc[:,i+36]/(train.iloc[:,i+37]+1e-20)\n",
    "    d.iloc[:,i-1] = test.iloc[:,i+36]/(test.iloc[:,i+37]+1e-20)\n",
    "train = pd.concat([train,a],axis=1)\n",
    "train = pd.concat([train,c],axis=1)\n",
    "test = pd.concat([test,b],axis=1)\n",
    "test = pd.concat([test,d],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src 및 dst의 값 행정규화 시행 \n",
    "srstd_list=['65srstd', '66srstd', '67srstd', '68srstd', '69srstd', '70srstd', '71srstd', '72srstd', '73srstd', \n",
    "          '74srstd', '75srstd', '76srstd', '77srstd', '78srstd', '79srstd', '80srstd', '81srstd', '82srstd', \n",
    "          '83srstd', '84srstd', '85srstd', '86srstd', '87srstd', '88srstd', '89srstd', '90srstd', '91srstd', \n",
    "          '92srstd', '93srstd', '94srstd', '95srstd', '96srstd', '97srstd', '98srstd', '99srstd','65dststd',\n",
    "          '66dststd', '67dststd', '68dststd', '69dststd', '70dststd', '71dststd', '72dststd', '73dststd', \n",
    "          '74dststd', '75dststd', '76dststd', '77dststd', '78dststd', '79dststd', '80dststd', '81dststd', '82dststd', \n",
    "          '83dststd', '84dststd', '85dststd', '86dststd', '87dststd', '88dststd', '89dststd', '90dststd', '91dststd', \n",
    "          '92dststd', '93dststd', '94dststd', '95dststd', '96dststd', '97dststd', '98dststd', '99dststd']\n",
    "\n",
    "mean_1 = train.iloc[:,1:36].mean(axis=1)\n",
    "mean_2 = train.iloc[:,36:71].mean(axis=1)\n",
    "\n",
    "std_1 = train.iloc[:,1:36].std(axis=1)\n",
    "std_2 = train.iloc[:,36:71].std(axis=1)\n",
    "\n",
    "mean_1t = test.iloc[:,1:36].mean(axis=1)\n",
    "mean_2t = test.iloc[:,36:71].mean(axis=1)\n",
    "\n",
    "std_1t = test.iloc[:,1:36].std(axis=1)\n",
    "std_2t = test.iloc[:,36:71].std(axis=1)\n",
    "\n",
    "tra_1 = train.iloc[:,1:36].apply(lambda x: (x-mean_1)/std_1)\n",
    "tra_2 = train.iloc[:,36:71].apply(lambda x: (x-mean_2)/std_2)\n",
    "tra = pd.concat([tra_1,tra_2],axis=1)\n",
    "tra.columns = srstd_list\n",
    "\n",
    "tes_1 = test.iloc[:,1:36].apply(lambda x: (x-mean_1t)/std_1t)\n",
    "tes_2 = test.iloc[:,36:71].apply(lambda x: (x-mean_2t)/std_2t)\n",
    "tes = pd.concat([tes_1,tes_2],axis=1)\n",
    "tes.columns = srstd_list\n",
    "\n",
    "train = pd.concat([train,tra],axis=1)\n",
    "test = pd.concat([test,tes],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src, dst의 값 행 minmax scaling 시행\n",
    "srmax_list=['65srmax', '66srmax', '67srmax', '68srmax', '69srmax', '70srmax', '71srmax', '72srmax', '73srmax', \n",
    "          '74srmax', '75srmax', '76srmax', '77srmax', '78srmax', '79srmax', '80srmax', '81srmax', '82srmax', \n",
    "          '83srmax', '84srmax', '85srmax', '86srmax', '87srmax', '88srmax', '89srmax', '90srmax', '91srmax', \n",
    "          '92srmax', '93srmax', '94srmax', '95srmax', '96srmax', '97srmax', '98srmax', '99srmax','65dstmax',\n",
    "          '66dstmax', '67dstmax', '68dstmax', '69dstmax', '70dstmax', '71dstmax', '72dstmax', '73dstmax', \n",
    "          '74dstmax', '75dstmax', '76dstmax', '77dstmax', '78dstmax', '79dstmax', '80dstmax', '81dstmax', '82dstmax', \n",
    "          '83dstmax', '84dstmax', '85dstmax', '86dstmax', '87dstmax', '88dstmax', '89dstmax', '90dstmax', '91dstmax', \n",
    "          '92dstmax', '93dstmax', '94dstmax', '95dstmax', '96dstmax', '97dstmax', '98dstmax', '99dstmax']\n",
    "\n",
    "max_1 = train.iloc[:,1:36].max(axis=1)\n",
    "max_2 = train.iloc[:,36:71].max(axis=1)\n",
    "\n",
    "min_1 = train.iloc[:,1:36].min(axis=1)\n",
    "min_2 = train.iloc[:,36:71].min(axis=1)\n",
    "\n",
    "max_1t = test.iloc[:,1:36].max(axis=1)\n",
    "max_2t = test.iloc[:,36:71].max(axis=1)\n",
    "\n",
    "min_1t = test.iloc[:,1:36].min(axis=1)\n",
    "min_2t = test.iloc[:,36:71].min(axis=1)\n",
    "\n",
    "tra_1 = train.iloc[:,1:36].apply(lambda x: (x-min_1)/(max_1 - min_1))\n",
    "tra_2 = train.iloc[:,36:71].apply(lambda x: (x-min_2)/(max_2 -min_2))\n",
    "tra = pd.concat([tra_1,tra_2],axis=1)\n",
    "tra.columns = srmax_list\n",
    "\n",
    "tes_1 = test.iloc[:,1:36].apply(lambda x: (x-min_1t)/(max_1t - min_1t))\n",
    "tes_2 = test.iloc[:,36:71].apply(lambda x: (x-min_2t)/(max_2t - min_2t))\n",
    "tes = pd.concat([tes_1,tes_2],axis=1)\n",
    "tes.columns = srmax_list\n",
    "\n",
    "train = pd.concat([train,tra],axis=1)\n",
    "test = pd.concat([test,tes],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src, dst값을 최대값으로 나눈 값 삽입 \n",
    "srmaratio_list=['65srmaratio', '66srmaratio', '67srmaratio', '68srmaratio', '69srmaratio', '70srmaratio', '71srmaratio', '72srmaratio', '73srmaratio', \n",
    "          '74srmaratio', '75srmaratio', '76srmaratio', '77srmaratio', '78srmaratio', '79srmaratio', '80srmaratio', '81srmaratio', '82srmaratio', \n",
    "          '83srmaratio', '84srmaratio', '85srmaratio', '86srmaratio', '87srmaratio', '88srmaratio', '89srmaratio', '90srmaratio', '91srmaratio', \n",
    "          '92srmaratio', '93srmaratio', '94srmaratio', '95srmaratio', '96srmaratio', '97srmaratio', '98srmaratio', '99srmaratio','65dstmaratio',\n",
    "          '66dstmaratio', '67dstmaratio', '68dstmaratio', '69dstmaratio', '70dstmaratio', '71dstmaratio', '72dstmaratio', '73dstmaratio', \n",
    "          '74dstmaratio', '75dstmaratio', '76dstmaratio', '77dstmaratio', '78dstmaratio', '79dstmaratio', '80dstmaratio', '81dstmaratio', '82dstmaratio', \n",
    "          '83dstmaratio', '84dstmaratio', '85dstmaratio', '86dstmaratio', '87dstmaratio', '88dstmaratio', '89dstmaratio', '90dstmaratio', '91dstmaratio', \n",
    "          '92dstmaratio', '93dstmaratio', '94dstmaratio', '95dstmaratio', '96dstmaratio', '97dstmaratio', '98dstmaratio', '99dstmaratio']\n",
    "max_1 = train.iloc[:,1:36].max(axis=1)\n",
    "max_2 = train.iloc[:,36:71].max(axis=1)\n",
    "\n",
    "tra_1 = train.iloc[:,1:36].apply(lambda x: x/max_1)\n",
    "tra_2 = train.iloc[:,36:71].apply(lambda x: x/max_2)\n",
    "tra = pd.concat([tra_1,tra_2],axis=1)\n",
    "tra.columns = srmaratio_list\n",
    "\n",
    "max_1t = test.iloc[:,1:36].max(axis=1)\n",
    "max_2t = test.iloc[:,36:71].max(axis=1)\n",
    "\n",
    "tra_1 = test.iloc[:,1:36].apply(lambda x: x/max_1t)\n",
    "tra_2 = test.iloc[:,36:71].apply(lambda x: x/max_2t)\n",
    "tes = pd.concat([tra_1,tra_2],axis=1)\n",
    "tes.columns = srmaratio_list\n",
    "\n",
    "train = pd.concat([train,tra],axis=1)\n",
    "test = pd.concat([test,tes],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src, dst의 최대값 최소값 평균, 표준편자, 최대값 최소값 차이, src, dst의 최대값 최소값 차이의 비율 삽입\n",
    "train['src_max'] = train.iloc[:,1:36].max(axis=1)\n",
    "train['src_min'] = train.iloc[:,1:36].min(axis=1)\n",
    "train['dst_max'] = train.iloc[:,36:71].max(axis=1)\n",
    "train['dst_min'] = train.iloc[:,36:71].min(axis=1)\n",
    "train['src_mean'] = train.iloc[:,1:36].mean(axis=1)\n",
    "train['dst_mean'] = train.iloc[:,36:71].mean(axis=1)\n",
    "train['src_std'] = train.iloc[:,1:36].std(axis=1)\n",
    "train['dst_std'] = train.iloc[:,36:71].std(axis=1)\n",
    "train['max_min_src'] = train['src_max'] - train['src_min']\n",
    "train['max_min_dst'] = train['dst_max'] - train['dst_min']\n",
    "train['dst_src_max_min_ratio'] = train['max_min_src'] / (train['max_min_dst']+0.01)\n",
    "test['src_max'] = test.iloc[:,1:36].max(axis=1)\n",
    "test['src_min'] = test.iloc[:,1:36].min(axis=1)\n",
    "test['dst_max'] = test.iloc[:,36:71].max(axis=1)\n",
    "test['dst_min'] = test.iloc[:,36:71].min(axis=1)\n",
    "test['src_mean'] = test.iloc[:,1:36].mean(axis=1)\n",
    "test['dst_mean'] = test.iloc[:,36:71].mean(axis=1)\n",
    "test['src_std'] = test.iloc[:,1:36].std(axis=1)\n",
    "test['dst_std'] = test.iloc[:,36:71].std(axis=1)\n",
    "test['max_min_src'] = test['src_max'] - test['src_min']\n",
    "test['max_min_dst'] = test['dst_max'] - test['dst_min']\n",
    "test['dst_src_max_min_ratio'] = test['max_min_src'] / (test['max_min_dst']+0.01)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srmul_list=['65srmul', '66srmul', '67srmul', '68srmul', '69srmul', '70srmul', '71srmul', '72srmul', '73srmul', \n",
    "          '74srmul', '75srmul', '76srmul', '77srmul', '78srmul', '79srmul', '80srmul', '81srmul', '82srmul', \n",
    "          '83srmul', '84srmul', '85srmul', '86srmul', '87srmul', '88srmul', '89srmul', '90srmul', '91srmul', \n",
    "          '92srmul', '93srmul', '94srmul', '95srmul', '96srmul', '97srmul', '98srmul', '99srmul','65dstmul',\n",
    "          '66dstmul', '67dstmul', '68dstmul', '69dstmul', '70dstmul', '71dstmul', '72dstmul', '73dstmul', \n",
    "          '74dstmul', '75dstmul', '76dstmul', '77dstmul', '78dstmul', '79dstmul', '80dstmul', '81dstmul', '82dstmul', \n",
    "          '83dstmul', '84dstmul', '85dstmul', '86dstmul', '87dstmul', '88dstmul', '89dstmul', '90dstmul', '91dstmul', \n",
    "          '92dstmul', '93dstmul', '94dstmul', '95dstmul', '96dstmul', '97dstmul', '98dstmul', '99dstmul']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rho의 거리 제곱을 곱하여 거리와 상관없는 빛의 세기로 만듦.\n",
    "a = train.iloc[:,1:71].apply(lambda x: x*(train['rho']^2))\n",
    "a.columns = srmul_list\n",
    "b = test.iloc[:,1:71].apply(lambda x: x*(test['rho']^2))\n",
    "b.columns = srmul_list\n",
    "\n",
    "train =pd.concat([train,a],axis=1)\n",
    "test =pd.concat([test,b],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srmui_list=['65srmui', '66srmui', '67srmui', '68srmui', '69srmui', '70srmui', '71srmui', '72srmui', '73srmui', \n",
    "          '74srmui', '75srmui', '76srmui', '77srmui', '78srmui', '79srmui', '80srmui', '81srmui', '82srmui', \n",
    "          '83srmui', '84srmui', '85srmui', '86srmui', '87srmui', '88srmui', '89srmui', '90srmui', '91srmui', \n",
    "          '92srmui', '93srmui', '94srmui', '95srmui', '96srmui', '97srmui', '98srmui', '99srmui']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src와 거리의 제곱을 곱한 후 dst를 곱하는 피쳐를 만듦.\n",
    "a = train.iloc[:,1:36].apply(lambda x: x*(train['rho']^2)).values*train.iloc[:,36:71].values\n",
    "a = pd.DataFrame(a,columns=srmui_list)\n",
    "b = test.iloc[:,1:36].apply(lambda x: x*(test['rho']^2)).values*test.iloc[:,36:71].values\n",
    "b = pd.DataFrame(b,columns=srmui_list)\n",
    "\n",
    "train =pd.concat([train,a],axis=1)\n",
    "test =pd.concat([test,b],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src에 거리제곱을 나누어 거리와 관계없는 빛의 세기로 만들고 dst를 곱함\n",
    "srmuii_list=['65srmuii', '66srmuii', '67srmuii', '68srmuii', '69srmuii', '70srmuii', '71srmuii', '72srmuii', '73srmuii', \n",
    "          '74srmuii', '75srmuii', '76srmuii', '77srmuii', '78srmuii', '79srmuii', '80srmuii', '81srmuii', '82srmuii', \n",
    "          '83srmuii', '84srmuii', '85srmuii', '86srmuii', '87srmuii', '88srmuii', '89srmuii', '90srmuii', '91srmuii', \n",
    "          '92srmuii', '93srmuii', '94srmuii', '95srmuii', '96srmuii', '97srmuii', '98srmuii', '99srmuii']\n",
    "\n",
    "a = train.iloc[:,1:36].apply(lambda x: x/(train['rho']^2)).values*train.iloc[:,36:71].values\n",
    "a = pd.DataFrame(a,columns=srmuii_list)\n",
    "b = test.iloc[:,1:36].apply(lambda x: x/(test['rho']^2)).values*test.iloc[:,36:71].values\n",
    "b = pd.DataFrame(b,columns=srmuii_list)\n",
    "\n",
    "train =pd.concat([train,a],axis=1)\n",
    "test =pd.concat([test,b],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgbm 모델을 돌리는데 dart를 사용 많은 양을 학습시킨 후 최소의 mae num_rounds를 찾아내 재학습 시행 이를 반복함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_cv, y, y_cv = train_test_split(train.values,train_y['hhb'], test_size=0.15, random_state=14414)\n",
    "\n",
    "def lgbm_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=288, num_rounds=46741):\n",
    "\n",
    "    param = {}\n",
    "\n",
    "    param['boosting'] = 'dart'\n",
    "    \n",
    "    param['objective'] = 'regression'\n",
    "\n",
    "    param['learning_rate'] = 0.05\n",
    "\n",
    "    param['max_depth'] = 10\n",
    "\n",
    "    param['metric'] = 'mae'\n",
    "    \n",
    "    param['is_training_metric'] = True\n",
    "    \n",
    "    param['min_child_weight'] = 1\n",
    "\n",
    "    param['bagging_fraction'] = 0.8\n",
    "    \n",
    "    param['num_leaves'] = 128\n",
    "\n",
    "    param['feature_fraction'] = 0.8\n",
    "\n",
    "    param['bagging_freq'] = 6\n",
    "    \n",
    "    param['seed'] = seed_val\n",
    "    \n",
    "    param['min_split_gain'] = 0.01\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    test_ds = lgb.Dataset(test_X, label=test_y)\n",
    "\n",
    "    model = lgb.train(param, train_ds, num_rounds,test_ds, early_stopping_rounds=180)\n",
    "\n",
    "    return model\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "\n",
    "model = lgbm_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =  model.predict(test.values)\n",
    "sample['hhb'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_cv, y, y_cv = train_test_split(train.values,train_y['hbo2'], test_size=0.15, random_state=14414)\n",
    "\n",
    "def lgbm_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=288, num_rounds=29668):\n",
    "\n",
    "    param = {}\n",
    "\n",
    "    param['boosting'] = 'dart'\n",
    "    \n",
    "    param['objective'] = 'regression'\n",
    "\n",
    "    param['learning_rate'] = 0.05\n",
    "\n",
    "    param['max_depth'] = 10\n",
    "\n",
    "    param['metric'] = 'mae'\n",
    "    \n",
    "    param['is_training_metric'] = True\n",
    "    \n",
    "    param['min_child_weight'] = 1\n",
    "\n",
    "    param['bagging_fraction'] = 0.8\n",
    "    \n",
    "    param['num_leaves'] = 128\n",
    "\n",
    "    param['feature_fraction'] = 0.8\n",
    "\n",
    "    param['bagging_freq'] = 6\n",
    "    \n",
    "    param['seed'] = seed_val\n",
    "    \n",
    "    param['min_split_gain'] = 0.01\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    test_ds = lgb.Dataset(test_X, label=test_y)\n",
    "\n",
    "    model = lgb.train(param, train_ds, num_rounds,test_ds, early_stopping_rounds=180)\n",
    "\n",
    "    return model\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "\n",
    "model = lgbm_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =  model.predict(test.values)\n",
    "sample['hbo2'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_cv, y, y_cv = train_test_split(train.values,train_y['ca'], test_size=0.15, random_state=12300)\n",
    "\n",
    "def lgbm_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=288, num_rounds=29652):\n",
    "\n",
    "    param = {}\n",
    "\n",
    "    param['boosting'] = 'dart'\n",
    "    \n",
    "    param['objective'] = 'regression'\n",
    "\n",
    "    param['learning_rate'] = 0.05\n",
    "\n",
    "    param['max_depth'] = 10\n",
    "\n",
    "    param['metric'] = 'mae'\n",
    "    \n",
    "    param['is_training_metric'] = True\n",
    "    \n",
    "    param['min_child_weight'] = 1\n",
    "\n",
    "    param['bagging_fraction'] = 0.8\n",
    "    \n",
    "    param['num_leaves'] = 128\n",
    "\n",
    "    param['feature_fraction'] = 0.8\n",
    "\n",
    "    param['bagging_freq'] = 6\n",
    "    \n",
    "    param['seed'] = seed_val\n",
    "    \n",
    "    param['min_split_gain'] = 0.01\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    test_ds = lgb.Dataset(test_X, label=test_y)\n",
    "\n",
    "    model = lgb.train(param, train_ds, num_rounds,test_ds, early_stopping_rounds=180)\n",
    "\n",
    "    return model\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "model = lgbm_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =  model.predict(test.values)\n",
    "sample['ca'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_cv, y, y_cv = train_test_split(train.values,train_y['na'], test_size=0.15, random_state=12300)\n",
    "\n",
    "def lgbm_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=288, num_rounds=12017):\n",
    "\n",
    "    param = {}\n",
    "\n",
    "    param['boosting'] = 'dart'\n",
    "    \n",
    "    param['objective'] = 'regression'\n",
    "\n",
    "    param['learning_rate'] = 0.05\n",
    "\n",
    "    param['max_depth'] = 10\n",
    "\n",
    "    param['metric'] = 'mae'\n",
    "    \n",
    "    param['is_training_metric'] = True\n",
    "    \n",
    "    param['min_child_weight'] = 1\n",
    "\n",
    "    param['bagging_fraction'] = 0.8\n",
    "    \n",
    "    param['num_leaves'] = 128\n",
    "\n",
    "    param['feature_fraction'] = 0.8\n",
    "\n",
    "    param['bagging_freq'] = 6\n",
    "    \n",
    "    param['seed'] = seed_val\n",
    "    \n",
    "    param['min_split_gain'] = 0.01\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    test_ds = lgb.Dataset(test_X, label=test_y)\n",
    "\n",
    "    model = lgb.train(param, train_ds, num_rounds,test_ds, early_stopping_rounds=180)\n",
    "\n",
    "    return model\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "model = lgbm_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =  model.predict(test.values)\n",
    "sample['na'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 값에 제대로 값이 삽입 됫는지 확인 - 음수 값이 존재함을 볼 수 있음\n",
    "sample.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음수 값은 있을 수 없기 때문에 음수값은 전부 0으로 치환\n",
    "def zero_fill(x):\n",
    "    if x <=0:\n",
    "        return .0\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['hhb'] = sample['hhb'].map(zero_fill)\n",
    "sample['na'] = sample['na'].map(zero_fill)\n",
    "sample.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv('Making.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 6\n",
    "\n",
    "## 필요한 패키지 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최종 제출 형태 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv('sample_submission.csv',engine='python')\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv',engine='python').iloc[:,1:]\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv',engine='python').iloc[:,1:]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터에 존재하는 예측하고자 하는 값 추출 및 제거\n",
    "train_y = train.iloc[:,-4:]\n",
    "train = train.iloc[:,:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train data에 존재하는 Null값 확인\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dacon에 있는 결측치 처리방법 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dst data에만 Null값이 있는것을 확인하여 dst만 추출\n",
    "train_dst = train.filter(regex='_dst$', axis=1).replace(0, np.NaN) # dst 데이터만 따로 뺀다.\n",
    "test_dst = test.filter(regex='_dst$', axis=1).replace(0, np.NaN) # 보간을 하기위해 결측값을 삭제한다.\n",
    "test_dst.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 보간법 방법중 linear 방법을 활용하여 NaN값 삽입 \n",
    "train_dst = train_dst.interpolate(methods='linear', axis=1)\n",
    "test_dst = test_dst.interpolate(methods='linear', axis=1)\n",
    "# 스팩트럼 데이터에서 연속해서 NaN이 있는 경우 처리가 안되기 때문에 이러한 값은 전부 0으로 처리\n",
    "train_dst.fillna(0, inplace=True) \n",
    "test_dst.fillna(0, inplace=True)\n",
    "test_dst.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.update(train_dst) # 보간한 데이터를 기존 데이터프레임에 업데이트 한다.\n",
    "test.update(test_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 다 채워진 모습을 볼 수 있다\n",
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src와 dst간 사칙연산 결과 삽입. 어느정도 성능의 향상 존재 + dst가 0인 경우가 있기 때문에 1e-18 추가해서 계산.\n",
    "\n",
    "add_list=['650_add', '660_add', '670_add', '680_add', '690_add', '700_add', '710_add', '720_add', '730_add', \n",
    "          '740_add', '750_add', '760_add', '770_add', '780_add', '790_add', '800_add', '810_add', '820_add', \n",
    "          '830_add', '840_add', '850_add', '860_add', '870_add', '880_add', '890_add', '900_add', '910_add', \n",
    "          '920_add', '930_add', '940_add', '950_add', '960_add', '970_add', '980_add', '990_add']\n",
    "diff_list=['650_diff', '660_diff', '670_diff', '680_diff', '690_diff', '700_diff', '710_diff', '720_diff', '730_diff', \n",
    "          '740_diff', '750_diff', '760_diff', '770_diff', '780_diff', '790_diff', '800_diff', '810_diff', '820_diff', \n",
    "          '830_diff', '840_diff', '850_diff', '860_diff', '870_diff', '880_diff', '890_diff', '900_diff', '910_diff', \n",
    "          '920_diff', '930_diff', '940_diff', '950_diff', '960_diff', '970_diff', '980_diff', '990_diff']\n",
    "div_list=['650_div', '660_div', '670_div', '680_div', '690_div', '700_div', '710_div', '720_div', '730_div', \n",
    "          '740_div', '750_div', '760_div', '770_div', '780_div', '790_div', '800_div', '810_div', '820_div', \n",
    "          '830_div', '840_div', '850_div', '860_div', '870_div', '880_div', '890_div', '900_div', '910_div', \n",
    "          '920_div', '930_div', '940_div', '950_div', '960_div', '970_div', '980_div', '990_div']\n",
    "multi_list=['650_multi', '660_multi', '670_multi', '680_multi', '690_multi', '700_multi', '710_multi', '720_multi', '730_multi', \n",
    "          '740_multi', '750_multi', '760_multi', '770_multi', '780_multi', '790_multi', '800_multi', '810_multi', '820_multi', \n",
    "          '830_multi', '840_multi', '850_multi', '860_multi', '870_multi', '880_multi', '890_multi', '900_multi', '910_multi', \n",
    "          '920_multi', '930_multi', '940_multi', '950_multi', '960_multi', '970_multi', '980_multi', '990_multi']\n",
    "dst_src_add = pd.DataFrame(train.iloc[:,1:36].values + train.iloc[:,36:71].values,columns=add_list)\n",
    "dst_src_diff = pd.DataFrame(train.iloc[:,1:36].values - train.iloc[:,36:71].values,columns=diff_list)\n",
    "dst_src_div = pd.DataFrame(train.iloc[:,1:36].values / (train.iloc[:,36:71].values+1e-18),columns=div_list)\n",
    "dst_src_mul = pd.DataFrame(train.iloc[:,1:36].values * train.iloc[:,36:71].values,columns=multi_list)\n",
    "train = pd.concat([train,dst_src_add],axis=1)\n",
    "train = pd.concat([train,dst_src_diff],axis=1)\n",
    "train = pd.concat([train,dst_src_div],axis=1)\n",
    "train = pd.concat([train,dst_src_mul],axis=1)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_src_add = pd.DataFrame(test.iloc[:,1:36].values + test.iloc[:,36:71].values,columns=add_list)\n",
    "dst_src_diff = pd.DataFrame(test.iloc[:,1:36].values - test.iloc[:,36:71].values,columns=diff_list)\n",
    "dst_src_mul = pd.DataFrame(test.iloc[:,1:36].values * test.iloc[:,36:71].values,columns=multi_list)\n",
    "dst_src_div = pd.DataFrame(test.iloc[:,1:36].values / (test.iloc[:,36:71].values+1e-16),columns=div_list)\n",
    "test = pd.concat([test,dst_src_add],axis=1)\n",
    "test = pd.concat([test,dst_src_diff],axis=1)\n",
    "test = pd.concat([test,dst_src_div],axis=1)\n",
    "test = pd.concat([test,dst_src_mul],axis=1)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 퓨리에 변환 실시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_list=['650_src', '660_src', '670_src', '680_src', '690_src', '700_src', '710_src', '720_src', '730_src', \n",
    "          '740_src', '750_src', '760_src', '770_src', '780_src', '790_src', '800_src', '810_src', '820_src', \n",
    "          '830_src', '840_src', '850_src', '860_src', '870_src', '880_src', '890_src', '900_src', '910_src', \n",
    "          '920_src', '930_src', '940_src', '950_src', '960_src', '970_src', '980_src', '990_src']\n",
    "\n",
    "dst_list=['650_dst', '660_dst', '670_dst', '680_dst', '690_dst', '700_dst', '710_dst', '720_dst', '730_dst', \n",
    "          '740_dst', '750_dst', '760_dst', '770_dst', '780_dst', '790_dst', '800_dst', '810_dst', '820_dst', \n",
    "          '830_dst', '840_dst', '850_dst', '860_dst', '870_dst', '880_dst', '890_dst', '900_dst', '910_dst', \n",
    "          '920_dst', '930_dst', '940_dst', '950_dst', '960_dst', '970_dst', '980_dst', '990_dst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alpha_real=train[dst_list]\n",
    "alpha_imag=train[dst_list]\n",
    "\n",
    "beta_real=test[dst_list]\n",
    "beta_imag=test[dst_list]\n",
    "\n",
    "for i in tqdm(alpha_real.index):\n",
    "    alpha_real.loc[i]=alpha_real.loc[i] - alpha_real.loc[i].mean()\n",
    "    alpha_imag.loc[i]=alpha_imag.loc[i] - alpha_real.loc[i].mean()\n",
    "    \n",
    "    alpha_real.loc[i] = np.fft.fft(alpha_real.loc[i], norm='ortho').real\n",
    "    alpha_imag.loc[i] = np.fft.fft(alpha_imag.loc[i], norm='ortho').imag\n",
    "\n",
    "    \n",
    "for i in tqdm(beta_real.index):\n",
    "    beta_real.loc[i]=beta_real.loc[i] - beta_real.loc[i].mean()\n",
    "    beta_imag.loc[i]=beta_imag.loc[i] - beta_imag.loc[i].mean()\n",
    "    \n",
    "    beta_real.loc[i] = np.fft.fft(beta_real.loc[i], norm='ortho').real\n",
    "    beta_imag.loc[i] = np.fft.fft(beta_imag.loc[i], norm='ortho').imag\n",
    "    \n",
    "real_part=[]\n",
    "imag_part=[]\n",
    "\n",
    "for col in dst_list:\n",
    "    real_part.append(col + '_fft_real')\n",
    "    imag_part.append(col + '_fft_imag')\n",
    "    \n",
    "alpha_real.columns=real_part\n",
    "alpha_imag.columns=imag_part\n",
    "alpha = pd.concat((alpha_real, alpha_imag), axis=1)\n",
    "\n",
    "beta_real.columns=real_part\n",
    "beta_imag.columns=imag_part\n",
    "beta=pd.concat((beta_real, beta_imag), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.concat((train, alpha), axis=1)\n",
    "test=pd.concat((test, beta), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del alpha, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 흡광계수 만들기 (빛의 흡수율 활용)\n",
    "\n",
    "src는 광원의 빛의 세기 dst는 측정된 빛의 세기 활용하여 흡광계수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(650,1000,10) : \n",
    "    train['흡수계수_{}'.format(i)] = np.log10(train['{}_src'.format(i)] / train['{}_dst'.format(i)]) / train.rho\n",
    "#     train['투과율_{}'.format(i)] = np.log10(train['{}_src'.format(i)] / train['{}_dst'.format(i)])\n",
    "    test['흡수계수_{}'.format(i)] = np.log10(test['{}_src'.format(i)] / test['{}_dst'.format(i)]) / test.rho\n",
    "#     test['투과율_{}'.format(i)] = np.log10(test['{}_src'.format(i)] / test['{}_dst'.format(i)])\n",
    "    \n",
    "train = train.fillna(0)\n",
    "test = test.fillna(0)\n",
    "\n",
    "train.isnull().sum().sum(), test.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 흡수량 만들어 삽입\n",
    "\n",
    "src의 실제 세기 대비 흡수된 빛의 세기를 계산하여 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srrho_lisrho = ['65_srrho', '66_srrho', '67_srrho', '68_srrho', '69_srrho', '70_srrho', '71_srrho', '72_srrho', '73_srrho', \n",
    "          '74_srrho', '75_srrho', '76_srrho', '77_srrho', '78_srrho', '79_srrho', '80_srrho', '81_srrho', '82_srrho', \n",
    "          '83_srrho', '84_srrho', '85_srrho', '86_srrho', '87_srrho', '88_srrho', '89_srrho', '90_srrho', '91_srrho', \n",
    "          '92_srrho', '93_srrho', '94_srrho', '95_srrho', '96_srrho', '97_srrho', '98_srrho', '99_srrho','65_dsrho',\n",
    "          '66_dsrho', '67_dsrho', '68_dsrho', '69_dsrho', '70_dsrho', '71_dsrho', '72_dsrho', '73_dsrho', \n",
    "          '74_dsrho', '75_dsrho', '76_dsrho', '77_dsrho', '78_dsrho', '79_dsrho', '80_dsrho', '81_dsrho', '82_dsrho', \n",
    "          '83_dsrho', '84_dsrho', '85_dsrho', '86_dsrho', '87_dsrho', '88_dsrho', '89_dsrho', '90_dsrho', '91_dsrho', \n",
    "          '92_dsrho', '93_dsrho', '94_dsrho', '95_dsrho', '96_dsrho', '97_dsrho', '98_dsrho', '99_dsrho']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(650,1000,10) : \n",
    "    train['흡수량_{}'.format(i)] = (train['{}_src'.format(i)]-train['{}_dst'.format(i)])/(train['{}_src'.format(i)]+1e-20)\n",
    "    test['흡수량_{}'.format(i)] =  (test['{}_src'.format(i)]-test['{}_dst'.format(i)])/(test['{}_src'.format(i)]+1e-20)\n",
    "a = train.iloc[:,1:71].apply(lambda x: x/(train['rho']^2))\n",
    "b = test.iloc[:,1:71].apply(lambda x: x/(test['rho']^2))\n",
    "a.columns = srrho_lisrho\n",
    "b.columns = srrho_lisrho\n",
    "train = pd.concat([train,a],axis=1)\n",
    "test = pd.concat([test,b],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이렇게 구한 흡수계수 활용 흡수평균 및 편차 합 변수 만듦\n",
    "train['흡수평균'] = train.iloc[:,281:316].replace(np.inf,1).replace(-np.inf,-1).mean(axis=1)\n",
    "train['흡수편차'] = train.iloc[:,281:316].replace(np.inf,1).replace(-np.inf,-1).std(axis=1)\n",
    "train['흡수합'] = train.iloc[:,281:316].replace(np.inf,1).replace(-np.inf,-1).sum(axis=1)\n",
    "test['흡수평균'] = test.iloc[:,281:316].replace(np.inf,1).replace(-np.inf,-1).mean(axis=1)\n",
    "test['흡수편차'] = test.iloc[:,281:316].replace(np.inf,1).replace(-np.inf,-1).std(axis=1)\n",
    "test['흡수합'] = test.iloc[:,281:316].replace(np.inf,1).replace(-np.inf,-1).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 흡수계수의 값으로 곱하여 새로운 피쳐 생성\n",
    "srr_liss=['65srrr', '66srrr', '67srrr', '68srrr', '69srrr', '70srrr', '71rsrr', '72srrr', '73srrr', \n",
    "          '74srrr', '75srrr', '76srrr', '77srrr', '78srrr', '79srrr', '80srrr', '81srrr', '82srrr', \n",
    "          '83srrr', '84srrr', '85srrr', '86srrr', '87srrr', '88srrr', '89srrr', '90srrr', '91srrr', \n",
    "          '92srrr', '93srrr', '94srrr', '95srrr', '96srrr', '97srrr', '98srrr','99srrr']\n",
    "dsr_liss = ['65dsss','66dsss', '67dsss', '68dsss', '69dsss', '70dsss', '71dsss', '72dsss', '73dsss', \n",
    "          '74dsss', '75dsss', '76dsss', '77dsss', '78dsss', '79dsss', '80dsss', '81dsss', '82dsss', \n",
    "          '83dsss', '84dsss', '85dsss', '86dsss', '87dsss', '88dsss', '89dsss', '90dsss', '91dsss', \n",
    "          '92dsss', '93dsss', '94dsss', '95dsss', '96dsss', '97dsss', '98dsss','99dsss']\n",
    "\n",
    "a = pd.DataFrame(train.iloc[:,1:36].values * train.iloc[:,281:316].replace(np.inf,1).replace(-np.inf,-1).values,columns=srr_liss)\n",
    "c = pd.DataFrame(train.iloc[:,36:71].values * train.iloc[:,281:316].replace(np.inf,1).replace(-np.inf,-1).values,columns=dsr_liss)\n",
    "b = pd.DataFrame(test.iloc[:,1:36].values * test.iloc[:,281:316].replace(np.inf,1).replace(-np.inf,-1).values, columns = srr_liss)\n",
    "d = pd.DataFrame(test.iloc[:,36:71].values * test.iloc[:,281:316].replace(np.inf,1).replace(-np.inf,-1).values,columns = dsr_liss)\n",
    "\n",
    "train = pd.concat([train,a],axis=1)\n",
    "train = pd.concat([train,c],axis=1)\n",
    "test = pd.concat([test,b],axis=1)\n",
    "test = pd.concat([test,d],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 흡수계수의 값으로 나누어 새로운 피쳐 생성 \n",
    "srr_liss=['65srrr', '66srrr', '67srrr', '68srrr', '69srrr', '70srrr', '71rsrr', '72srrr', '73srrr', \n",
    "          '74srrr', '75srrr', '76srrr', '77srrr', '78srrr', '79srrr', '80srrr', '81srrr', '82srrr', \n",
    "          '83srrr', '84srrr', '85srrr', '86srrr', '87srrr', '88srrr', '89srrr', '90srrr', '91srrr', \n",
    "          '92srrr', '93srrr', '94srrr', '95srrr', '96srrr', '97srrr', '98srrr','99srrr']\n",
    "dsr_liss = ['65dsss','66dsss', '67dsss', '68dsss', '69dsss', '70dsss', '71dsss', '72dsss', '73dsss', \n",
    "          '74dsss', '75dsss', '76dsss', '77dsss', '78dsss', '79dsss', '80dsss', '81dsss', '82dsss', \n",
    "          '83dsss', '84dsss', '85dsss', '86dsss', '87dsss', '88dsss', '89dsss', '90dsss', '91dsss', \n",
    "          '92dsss', '93dsss', '94dsss', '95dsss', '96dsss', '97dsss', '98dsss','99dsss']\n",
    "\n",
    "a = pd.DataFrame(train.iloc[:,1:36].values / (train.iloc[:,281:316].replace(np.inf,1).replace(-np.inf,-1).values + 1e-20),columns=srr_liss)\n",
    "c = pd.DataFrame(train.iloc[:,36:71].values / (train.iloc[:,281:316].replace(np.inf,1).replace(-np.inf,-1).values+ 1e-20),columns=dsr_liss)\n",
    "b = pd.DataFrame(test.iloc[:,1:36].values / (test.iloc[:,281:316].replace(np.inf,1).replace(-np.inf,-1).values+ 1e-20), columns = srr_liss)\n",
    "d = pd.DataFrame(test.iloc[:,36:71].values / (test.iloc[:,281:316].replace(np.inf,1).replace(-np.inf,-1).values+ 1e-20),columns = dsr_liss)\n",
    "\n",
    "train = pd.concat([train,a],axis=1)\n",
    "train = pd.concat([train,c],axis=1)\n",
    "test = pd.concat([test,b],axis=1)\n",
    "test = pd.concat([test,d],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum().sum(),test.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srr_liss=['65srr', '66srr', '67srr', '68srr', '69srr', '70srr', '71srr', '72srr', '73srr', \n",
    "          '74srr', '75srr', '76srr', '77srr', '78srr', '79srr', '80srr', '81srr', '82srr', \n",
    "          '83srr', '84srr', '85srr', '86srr', '87srr', '88srr', '89srr', '90srr', '91srr', \n",
    "          '92srr', '93srr', '94srr', '95srr', '96srr', '97srr', '98srr']\n",
    "dsr_liss = ['65dss','66dss', '67dss', '68dss', '69dss', '70dss', '71dss', '72dss', '73dss', \n",
    "          '74dss', '75dss', '76dss', '77dss', '78dss', '79dss', '80dss', '81dss', '82dss', \n",
    "          '83dss', '84dss', '85dss', '86dss', '87dss', '88dss', '89dss', '90dss', '91dss', \n",
    "          '92dss', '93dss', '94dss', '95dss', '96dss', '97dss', '98dss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src, dst의 바로 다음 값으로 나누어 파장별 비율 계산 \n",
    "a = pd.DataFrame(columns = srr_liss, index=range(10000))\n",
    "b = pd.DataFrame(columns = srr_liss, index=range(10000))\n",
    "c = pd.DataFrame(columns = dsr_liss, index=range(10000))\n",
    "d = pd.DataFrame(columns = dsr_liss, index=range(10000))\n",
    "for i in range(1,35):\n",
    "    a.iloc[:,i-1] = train.iloc[:,i]/(train.iloc[:,i+1]+1e-20)\n",
    "    b.iloc[:,i-1] = test.iloc[:,i]/(test.iloc[:,i+1]+1e-20)\n",
    "    c.iloc[:,i-1] = train.iloc[:,i+36]/(train.iloc[:,i+37]+1e-20)\n",
    "    d.iloc[:,i-1] = test.iloc[:,i+36]/(test.iloc[:,i+37]+1e-20)\n",
    "train = pd.concat([train,a],axis=1)\n",
    "train = pd.concat([train,c],axis=1)\n",
    "test = pd.concat([test,b],axis=1)\n",
    "test = pd.concat([test,d],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum().sum(),test.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파장별 나누는 것의 sliding을 넓힘. 높은 성능 향상을 보임 \n",
    "for j in range(10,350,10) : \n",
    "    for i in range(650,1000-j,10) : \n",
    "        train['흡수nextrat{}_{}'.format(i,j)] = train['흡수계수_{}'.format(i+j)].replace(np.inf,1).replace(-np.inf,-1) / (train['흡수계수_{}'.format(i)].replace(np.inf,1).replace(-np.inf,-1)+1e-20)\n",
    "        train['srcnextrat{}_{}'.format(i,j)] = train['{}_src'.format(i+j)].replace(np.inf,1).replace(-np.inf,-1) / (train['{}_src'.format(i)].replace(np.inf,1).replace(-np.inf,-1)+1e-20)\n",
    "        train['dstnextrat{}_{}'.format(i,j)] = train['{}_dst'.format(i+j)].replace(np.inf,1).replace(-np.inf,-1) / (train['{}_dst'.format(i)].replace(np.inf,1).replace(-np.inf,-1)+1e-20)\n",
    "        test['흡수nextrat{}_{}'.format(i,j)] = test['흡수계수_{}'.format(i+j)].replace(np.inf,1).replace(-np.inf,-1) / (test['흡수계수_{}'.format(i)].replace(np.inf,1).replace(-np.inf,-1)+1e-20)\n",
    "        test['srcnextrat{}_{}'.format(i,j)] = test['{}_src'.format(i+j)].replace(np.inf,1).replace(-np.inf,-1) / (test['{}_src'.format(i)].replace(np.inf,1).replace(-np.inf,-1)+1e-20)\n",
    "        test['dstnextrat{}_{}'.format(i,j)] = test['{}_dst'.format(i+j)].replace(np.inf,1).replace(-np.inf,-1) / (test['{}_dst'.format(i)].replace(np.inf,1).replace(-np.inf,-1)+1e-20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum().sum(),test.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src 및 dst의 값 행정규화 시행 \n",
    "\n",
    "srstd_list=['65srstd', '66srstd', '67srstd', '68srstd', '69srstd', '70srstd', '71srstd', '72srstd', '73srstd', \n",
    "          '74srstd', '75srstd', '76srstd', '77srstd', '78srstd', '79srstd', '80srstd', '81srstd', '82srstd', \n",
    "          '83srstd', '84srstd', '85srstd', '86srstd', '87srstd', '88srstd', '89srstd', '90srstd', '91srstd', \n",
    "          '92srstd', '93srstd', '94srstd', '95srstd', '96srstd', '97srstd', '98srstd', '99srstd','65dststd',\n",
    "          '66dststd', '67dststd', '68dststd', '69dststd', '70dststd', '71dststd', '72dststd', '73dststd', \n",
    "          '74dststd', '75dststd', '76dststd', '77dststd', '78dststd', '79dststd', '80dststd', '81dststd', '82dststd', \n",
    "          '83dststd', '84dststd', '85dststd', '86dststd', '87dststd', '88dststd', '89dststd', '90dststd', '91dststd', \n",
    "          '92dststd', '93dststd', '94dststd', '95dststd', '96dststd', '97dststd', '98dststd', '99dststd']\n",
    "\n",
    "mean_1 = train.iloc[:,1:36].mean(axis=1)\n",
    "mean_2 = train.iloc[:,36:71].mean(axis=1)\n",
    "\n",
    "std_1 = train.iloc[:,1:36].std(axis=1)\n",
    "std_2 = train.iloc[:,36:71].std(axis=1)\n",
    "\n",
    "mean_1t = test.iloc[:,1:36].mean(axis=1)\n",
    "mean_2t = test.iloc[:,36:71].mean(axis=1)\n",
    "\n",
    "std_1t = test.iloc[:,1:36].std(axis=1)\n",
    "std_2t = test.iloc[:,36:71].std(axis=1)\n",
    "\n",
    "tra_1 = train.iloc[:,1:36].apply(lambda x: (x-mean_1)/std_1)\n",
    "tra_2 = train.iloc[:,36:71].apply(lambda x: (x-mean_2)/std_2)\n",
    "tra = pd.concat([tra_1,tra_2],axis=1)\n",
    "tra.columns = srstd_list\n",
    "\n",
    "tes_1 = test.iloc[:,1:36].apply(lambda x: (x-mean_1t)/std_1t)\n",
    "tes_2 = test.iloc[:,36:71].apply(lambda x: (x-mean_2t)/std_2t)\n",
    "tes = pd.concat([tes_1,tes_2],axis=1)\n",
    "tes.columns = srstd_list\n",
    "\n",
    "train = pd.concat([train,tra],axis=1)\n",
    "test = pd.concat([test,tes],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src, dst의 값 행 minmax scaling 시행\n",
    "\n",
    "srmax_list=['65srmax', '66srmax', '67srmax', '68srmax', '69srmax', '70srmax', '71srmax', '72srmax', '73srmax', \n",
    "          '74srmax', '75srmax', '76srmax', '77srmax', '78srmax', '79srmax', '80srmax', '81srmax', '82srmax', \n",
    "          '83srmax', '84srmax', '85srmax', '86srmax', '87srmax', '88srmax', '89srmax', '90srmax', '91srmax', \n",
    "          '92srmax', '93srmax', '94srmax', '95srmax', '96srmax', '97srmax', '98srmax', '99srmax','65dstmax',\n",
    "          '66dstmax', '67dstmax', '68dstmax', '69dstmax', '70dstmax', '71dstmax', '72dstmax', '73dstmax', \n",
    "          '74dstmax', '75dstmax', '76dstmax', '77dstmax', '78dstmax', '79dstmax', '80dstmax', '81dstmax', '82dstmax', \n",
    "          '83dstmax', '84dstmax', '85dstmax', '86dstmax', '87dstmax', '88dstmax', '89dstmax', '90dstmax', '91dstmax', \n",
    "          '92dstmax', '93dstmax', '94dstmax', '95dstmax', '96dstmax', '97dstmax', '98dstmax', '99dstmax']\n",
    "\n",
    "max_1 = train.iloc[:,1:36].max(axis=1)\n",
    "max_2 = train.iloc[:,36:71].max(axis=1)\n",
    "\n",
    "min_1 = train.iloc[:,1:36].min(axis=1)\n",
    "min_2 = train.iloc[:,36:71].min(axis=1)\n",
    "\n",
    "max_1t = test.iloc[:,1:36].max(axis=1)\n",
    "max_2t = test.iloc[:,36:71].max(axis=1)\n",
    "\n",
    "min_1t = test.iloc[:,1:36].min(axis=1)\n",
    "min_2t = test.iloc[:,36:71].min(axis=1)\n",
    "\n",
    "tra_1 = train.iloc[:,1:36].apply(lambda x: (x-min_1)/(max_1 - min_1))\n",
    "tra_2 = train.iloc[:,36:71].apply(lambda x: (x-min_2)/(max_2 -min_2))\n",
    "tra = pd.concat([tra_1,tra_2],axis=1)\n",
    "tra.columns = srmax_list\n",
    "\n",
    "tes_1 = test.iloc[:,1:36].apply(lambda x: (x-min_1t)/(max_1t - min_1t))\n",
    "tes_2 = test.iloc[:,36:71].apply(lambda x: (x-min_2t)/(max_2t - min_2t))\n",
    "tes = pd.concat([tes_1,tes_2],axis=1)\n",
    "tes.columns = srmax_list\n",
    "\n",
    "train = pd.concat([train,tra],axis=1)\n",
    "test = pd.concat([test,tes],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src, dst값을 최대값으로 나눈 값 삽입 \n",
    "\n",
    "srmaratio_list=['65srmaratio', '66srmaratio', '67srmaratio', '68srmaratio', '69srmaratio', '70srmaratio', '71srmaratio', '72srmaratio', '73srmaratio', \n",
    "          '74srmaratio', '75srmaratio', '76srmaratio', '77srmaratio', '78srmaratio', '79srmaratio', '80srmaratio', '81srmaratio', '82srmaratio', \n",
    "          '83srmaratio', '84srmaratio', '85srmaratio', '86srmaratio', '87srmaratio', '88srmaratio', '89srmaratio', '90srmaratio', '91srmaratio', \n",
    "          '92srmaratio', '93srmaratio', '94srmaratio', '95srmaratio', '96srmaratio', '97srmaratio', '98srmaratio', '99srmaratio','65dstmaratio',\n",
    "          '66dstmaratio', '67dstmaratio', '68dstmaratio', '69dstmaratio', '70dstmaratio', '71dstmaratio', '72dstmaratio', '73dstmaratio', \n",
    "          '74dstmaratio', '75dstmaratio', '76dstmaratio', '77dstmaratio', '78dstmaratio', '79dstmaratio', '80dstmaratio', '81dstmaratio', '82dstmaratio', \n",
    "          '83dstmaratio', '84dstmaratio', '85dstmaratio', '86dstmaratio', '87dstmaratio', '88dstmaratio', '89dstmaratio', '90dstmaratio', '91dstmaratio', \n",
    "          '92dstmaratio', '93dstmaratio', '94dstmaratio', '95dstmaratio', '96dstmaratio', '97dstmaratio', '98dstmaratio', '99dstmaratio']\n",
    "max_1 = train.iloc[:,1:36].max(axis=1)\n",
    "max_2 = train.iloc[:,36:71].max(axis=1)\n",
    "\n",
    "tra_1 = train.iloc[:,1:36].apply(lambda x: x/max_1)\n",
    "tra_2 = train.iloc[:,36:71].apply(lambda x: x/max_2)\n",
    "tra = pd.concat([tra_1,tra_2],axis=1)\n",
    "tra.columns = srmaratio_list\n",
    "\n",
    "max_1t = test.iloc[:,1:36].max(axis=1)\n",
    "max_2t = test.iloc[:,36:71].max(axis=1)\n",
    "\n",
    "tra_1 = test.iloc[:,1:36].apply(lambda x: x/max_1t)\n",
    "tra_2 = test.iloc[:,36:71].apply(lambda x: x/max_2t)\n",
    "tes = pd.concat([tra_1,tra_2],axis=1)\n",
    "tes.columns = srmaratio_list\n",
    "\n",
    "train = pd.concat([train,tra],axis=1)\n",
    "test = pd.concat([test,tes],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src, dst의 최대값 최소값 평균, 표준편자, 최대값 최소값 차이, src, dst의 최대값 최소값 차이의 비율 삽입\n",
    "\n",
    "train['src_max'] = train.iloc[:,1:36].max(axis=1)\n",
    "train['src_min'] = train.iloc[:,1:36].min(axis=1)\n",
    "train['dst_max'] = train.iloc[:,36:71].max(axis=1)\n",
    "train['dst_min'] = train.iloc[:,36:71].min(axis=1)\n",
    "train['src_mean'] = train.iloc[:,1:36].mean(axis=1)\n",
    "train['dst_mean'] = train.iloc[:,36:71].mean(axis=1)\n",
    "train['src_std'] = train.iloc[:,1:36].std(axis=1)\n",
    "train['dst_std'] = train.iloc[:,36:71].std(axis=1)\n",
    "train['max_min_src'] = train['src_max'] - train['src_min']\n",
    "train['max_min_dst'] = train['dst_max'] - train['dst_min']\n",
    "train['dst_src_max_min_ratio'] = train['max_min_src'] / (train['max_min_dst']+0.01)\n",
    "test['src_max'] = test.iloc[:,1:36].max(axis=1)\n",
    "test['src_min'] = test.iloc[:,1:36].min(axis=1)\n",
    "test['dst_max'] = test.iloc[:,36:71].max(axis=1)\n",
    "test['dst_min'] = test.iloc[:,36:71].min(axis=1)\n",
    "test['src_mean'] = test.iloc[:,1:36].mean(axis=1)\n",
    "test['dst_mean'] = test.iloc[:,36:71].mean(axis=1)\n",
    "test['src_std'] = test.iloc[:,1:36].std(axis=1)\n",
    "test['dst_std'] = test.iloc[:,36:71].std(axis=1)\n",
    "test['max_min_src'] = test['src_max'] - test['src_min']\n",
    "test['max_min_dst'] = test['dst_max'] - test['dst_min']\n",
    "test['dst_src_max_min_ratio'] = test['max_min_src'] / (test['max_min_dst']+0.01)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srmul_list=['65srmul', '66srmul', '67srmul', '68srmul', '69srmul', '70srmul', '71srmul', '72srmul', '73srmul', \n",
    "          '74srmul', '75srmul', '76srmul', '77srmul', '78srmul', '79srmul', '80srmul', '81srmul', '82srmul', \n",
    "          '83srmul', '84srmul', '85srmul', '86srmul', '87srmul', '88srmul', '89srmul', '90srmul', '91srmul', \n",
    "          '92srmul', '93srmul', '94srmul', '95srmul', '96srmul', '97srmul', '98srmul', '99srmul','65dstmul',\n",
    "          '66dstmul', '67dstmul', '68dstmul', '69dstmul', '70dstmul', '71dstmul', '72dstmul', '73dstmul', \n",
    "          '74dstmul', '75dstmul', '76dstmul', '77dstmul', '78dstmul', '79dstmul', '80dstmul', '81dstmul', '82dstmul', \n",
    "          '83dstmul', '84dstmul', '85dstmul', '86dstmul', '87dstmul', '88dstmul', '89dstmul', '90dstmul', '91dstmul', \n",
    "          '92dstmul', '93dstmul', '94dstmul', '95dstmul', '96dstmul', '97dstmul', '98dstmul', '99dstmul']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rho의 거리 제곱을 곱하여 거리와 상관없는 빛의 세기로 만듦.\n",
    "\n",
    "a = train.iloc[:,1:71].apply(lambda x: x*(train['rho']^2))\n",
    "a.columns = srmul_list\n",
    "b = test.iloc[:,1:71].apply(lambda x: x*(test['rho']^2))\n",
    "b.columns = srmul_list\n",
    "\n",
    "train =pd.concat([train,a],axis=1)\n",
    "test =pd.concat([test,b],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srmui_list=['65srmui', '66srmui', '67srmui', '68srmui', '69srmui', '70srmui', '71srmui', '72srmui', '73srmui', \n",
    "          '74srmui', '75srmui', '76srmui', '77srmui', '78srmui', '79srmui', '80srmui', '81srmui', '82srmui', \n",
    "          '83srmui', '84srmui', '85srmui', '86srmui', '87srmui', '88srmui', '89srmui', '90srmui', '91srmui', \n",
    "          '92srmui', '93srmui', '94srmui', '95srmui', '96srmui', '97srmui', '98srmui', '99srmui']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src와 거리의 제곱을 곱한 후 dst를 곱하는 피쳐를 만듦.\n",
    "a = train.iloc[:,1:36].apply(lambda x: x*(train['rho']^2)).values*train.iloc[:,36:71].values\n",
    "a = pd.DataFrame(a,columns=srmui_list)\n",
    "b = test.iloc[:,1:36].apply(lambda x: x*(test['rho']^2)).values*test.iloc[:,36:71].values\n",
    "b = pd.DataFrame(b,columns=srmui_list)\n",
    "\n",
    "train =pd.concat([train,a],axis=1)\n",
    "test =pd.concat([test,b],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src에 거리제곱을 나누어 거리와 관계없는 빛의 세기로 만들고 dst를 곱함\n",
    "srmuii_list=['65srmuii', '66srmuii', '67srmuii', '68srmuii', '69srmuii', '70srmuii', '71srmuii', '72srmuii', '73srmuii', \n",
    "          '74srmuii', '75srmuii', '76srmuii', '77srmuii', '78srmuii', '79srmuii', '80srmuii', '81srmuii', '82srmuii', \n",
    "          '83srmuii', '84srmuii', '85srmuii', '86srmuii', '87srmuii', '88srmuii', '89srmuii', '90srmuii', '91srmuii', \n",
    "          '92srmuii', '93srmuii', '94srmuii', '95srmuii', '96srmuii', '97srmuii', '98srmuii', '99srmuii']\n",
    "\n",
    "a = train.iloc[:,1:36].apply(lambda x: x/(train['rho']^2)).values*train.iloc[:,36:71].values\n",
    "a = pd.DataFrame(a,columns=srmuii_list)\n",
    "b = test.iloc[:,1:36].apply(lambda x: x/(test['rho']^2)).values*test.iloc[:,36:71].values\n",
    "b = pd.DataFrame(b,columns=srmuii_list)\n",
    "\n",
    "train =pd.concat([train,a],axis=1)\n",
    "test =pd.concat([test,b],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src의 값에 dst 값을 빼고 src로 나누어 흡수량 만듦.\n",
    "srmuii_list=['65srmuii', '66srmuii', '67srmuii', '68srmuii', '69srmuii', '70srmuii', '71srmuii', '72srmuii', '73srmuii', \n",
    "          '74srmuii', '75srmuii', '76srmuii', '77srmuii', '78srmuii', '79srmuii', '80srmuii', '81srmuii', '82srmuii', \n",
    "          '83srmuii', '84srmuii', '85srmuii', '86srmuii', '87srmuii', '88srmuii', '89srmuii', '90srmuii', '91srmuii', \n",
    "          '92srmuii', '93srmuii', '94srmuii', '95srmuii', '96srmuii', '97srmuii', '98srmuii', '99srmuii']\n",
    "\n",
    "a = pd.DataFrame((train.iloc[:,1:36].values - train.iloc[:,36:71].values)/(train.iloc[:,1:36].values+1e-20),columns=srmuii_list)\n",
    "b = pd.DataFrame((test.iloc[:,1:36].values - test.iloc[:,36:71].values)/(test.iloc[:,1:36].values+1e-20),columns=srmuii_list)\n",
    "train =pd.concat([train,a],axis=1)\n",
    "test =pd.concat([test,b],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lgbm 모델을 돌리는데 dart를 사용 많은 양을 학습시킨 후 최소의 mae num_rounds를 찾아내 재학습 시행 이를 반복함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_cv, y, y_cv = train_test_split(train.values,train_y['hhb'], test_size=0.15, random_state=14414)\n",
    "\n",
    "def lgbm_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=288, num_rounds=27068):\n",
    "\n",
    "    param = {}\n",
    "\n",
    "    param['boosting'] = 'dart'\n",
    "    \n",
    "    param['objective'] = 'regression'\n",
    "\n",
    "    param['learning_rate'] = 0.05\n",
    "\n",
    "    param['max_depth'] = 10\n",
    "\n",
    "    param['metric'] = 'mae'\n",
    "    \n",
    "    param['is_training_metric'] = True\n",
    "    \n",
    "    param['min_child_weight'] = 1\n",
    "\n",
    "    param['bagging_fraction'] = 0.8\n",
    "    \n",
    "    param['num_leaves'] = 128\n",
    "\n",
    "    param['feature_fraction'] = 0.8\n",
    "\n",
    "    param['bagging_freq'] = 6\n",
    "    \n",
    "    param['seed'] = seed_val\n",
    "    \n",
    "    param['min_split_gain'] = 0.01\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    test_ds = lgb.Dataset(test_X, label=test_y)\n",
    "\n",
    "    model = lgb.train(param, train_ds, num_rounds,test_ds, early_stopping_rounds=180)\n",
    "\n",
    "    return model\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "\n",
    "model = lgbm_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)\n",
    "\n",
    "# from matplotlib import pylab as plt\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(12,18))\n",
    "\n",
    "# lgb.plot_importance(model, max_num_featurs=50, height=0.8, ax=ax)\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =  model.predict(test.values)\n",
    "sample['hhb'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x, train_cv, y, y_cv = train_test_split(train.values,train_y['hbo2'], test_size=0.15, random_state=14414)\n",
    "\n",
    "def lgbm_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=288, num_rounds=20396):\n",
    "\n",
    "    param = {}\n",
    "\n",
    "    param['boosting'] = 'dart'\n",
    "    \n",
    "    param['objective'] = 'regression'\n",
    "\n",
    "    param['learning_rate'] = 0.05\n",
    "\n",
    "    param['max_depth'] = 10\n",
    "\n",
    "    param['metric'] = 'mae'\n",
    "    \n",
    "    param['is_training_metric'] = True\n",
    "    \n",
    "    param['min_child_weight'] = 1\n",
    "\n",
    "    param['bagging_fraction'] = 0.8\n",
    "    \n",
    "    param['num_leaves'] = 128\n",
    "\n",
    "    param['feature_fraction'] = 0.8\n",
    "\n",
    "    param['bagging_freq'] = 6\n",
    "    \n",
    "    param['seed'] = seed_val\n",
    "    \n",
    "    param['min_split_gain'] = 0.01\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    test_ds = lgb.Dataset(test_X, label=test_y)\n",
    "\n",
    "    model = lgb.train(param, train_ds, num_rounds,test_ds, early_stopping_rounds=180)\n",
    "\n",
    "    return model\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "\n",
    "model = lgbm_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =  model.predict(test.values)\n",
    "sample['hbo2'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_x, train_cv, y, y_cv = train_test_split(train.values,train_y['ca'], test_size=0.15, random_state=12300)\n",
    "\n",
    "def lgbm_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=288, num_rounds=43259):\n",
    "\n",
    "    param = {}\n",
    "\n",
    "    param['boosting'] = 'dart'\n",
    "    \n",
    "    param['objective'] = 'regression'\n",
    "\n",
    "    param['learning_rate'] = 0.05\n",
    "\n",
    "    param['max_depth'] = 10\n",
    "\n",
    "    param['metric'] = 'mae'\n",
    "    \n",
    "    param['is_training_metric'] = True\n",
    "    \n",
    "    param['min_child_weight'] = 1\n",
    "\n",
    "    param['bagging_fraction'] = 0.8\n",
    "    \n",
    "    param['num_leaves'] = 128\n",
    "\n",
    "    param['feature_fraction'] = 0.8\n",
    "\n",
    "    param['bagging_freq'] = 6\n",
    "    \n",
    "    param['seed'] = seed_val\n",
    "    \n",
    "    param['min_split_gain'] = 0.01\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    test_ds = lgb.Dataset(test_X, label=test_y)\n",
    "\n",
    "    model = lgb.train(param, train_ds, num_rounds,test_ds, early_stopping_rounds=180)\n",
    "\n",
    "    return model\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "model = lgbm_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =  model.predict(test.values)\n",
    "sample['ca'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_cv, y, y_cv = train_test_split(train.values,train_y['na'], test_size=0.15, random_state=12300)\n",
    "\n",
    "def lgbm_regressor(train_X, train_y, test_X, test_y, feature_names=None, seed_val=288, num_rounds=15248):\n",
    "\n",
    "    param = {}\n",
    "\n",
    "    param['boosting'] = 'dart'\n",
    "    \n",
    "    param['objective'] = 'regression'\n",
    "\n",
    "    param['learning_rate'] = 0.05\n",
    "\n",
    "    param['max_depth'] = 10\n",
    "\n",
    "    param['metric'] = 'mae'\n",
    "    \n",
    "    param['is_training_metric'] = True\n",
    "    \n",
    "    param['min_child_weight'] = 1\n",
    "\n",
    "    param['bagging_fraction'] = 0.8\n",
    "    \n",
    "    param['num_leaves'] = 128\n",
    "\n",
    "    param['feature_fraction'] = 0.8\n",
    "\n",
    "    param['bagging_freq'] = 6\n",
    "    \n",
    "    param['seed'] = seed_val\n",
    "    \n",
    "    param['min_split_gain'] = 0.01\n",
    "    \n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "\n",
    "    train_ds = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    test_ds = lgb.Dataset(test_X, label=test_y)\n",
    "\n",
    "    model = lgb.train(param, train_ds, num_rounds,test_ds, early_stopping_rounds=180)\n",
    "\n",
    "    return model\n",
    "\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "model = lgbm_regressor(train_X = train_x, train_y = y, test_X = train_cv, test_y = y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test =  model.predict(test.values)\n",
    "sample['na'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음수 값은 있을 수 없기 때문에 음수값은 전부 0으로 치환\n",
    "def zero_fill(x):\n",
    "    if x <=0:\n",
    "        return .0\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['hhb'] = sample['hhb'].map(zero_fill)\n",
    "sample['na'] = sample['na'].map(zero_fill)\n",
    "sample.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.to_csv('TaeHyun.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final CSV Making "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가중평균 기법 사용\n",
    "\n",
    "random_state 바꾼것들을 하나의 집단으로 다른 피쳐들로 만든 csv들을 하나의 집단으로 만듦 \n",
    "\n",
    "나온 결과값에 대하여 성능이 좋은 모델에 가중치를 더 두어 평균을 냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('edit_dst.csv')\n",
    "b = pd.read_csv('edit_dst_998.csv')\n",
    "c= pd.read_csv('edit_dst22882.csv')\n",
    "d= pd.read_csv('edit_dst_3402.csv')\n",
    "\n",
    "sub = (a+b+c+d)/4\n",
    "\n",
    "t1 = pd.read_csv('TaeHyun.csv')\n",
    "t2 = pd.read_csv('Making.csv')\n",
    "\n",
    "t = (t1+t2)/2\n",
    "\n",
    "sub2 = (0.7*sub) + (0.3 * t)\n",
    "\n",
    "sub2.id = submission.id\n",
    "\n",
    "sub2.to_csv('es_final.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
